{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjoUGDuyTdh5"
      },
      "source": [
        "In this notebook we will demonstrate how to interpret a Deep Learning Model using [LIME](https://github.com/marcotcr/lime)(local interpretable model-agnostic explanations), a python package for explaining machine learning classifiers. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVQdTkPzH-Y7",
        "outputId": "1d0dea01-69b6-4c3b-930d-5d0bf45b6c96"
      },
      "outputs": [],
      "source": [
        "# To install only the requirements of this notebook, uncomment the lines below and run this cell\n",
        "\n",
        "# ===========================\n",
        "\n",
        "# !pip install pandas==1.1.5\n",
        "# !pip install scikit-learn==0.21.3\n",
        "# !pip install lime==0.2.0.1\n",
        "# !pip install tensorflow==1.14.0\n",
        "# !pip install numpy==1.19.5\n",
        "# !pip install matplotlib==3.2.2\n",
        "# !pip install seaborn==0.11.1\n",
        "\n",
        "# ==========================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xQa4Mi5HH-Y8"
      },
      "outputs": [],
      "source": [
        "# To install the requirements for the entire chapter, uncomment the lines below and run this cell\n",
        "\n",
        "# ===========================\n",
        "\n",
        "# try:\n",
        "#     import google.colab\n",
        "#     !curl  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/ch4-requirements.txt | xargs -n 1 -L 1 pip install\n",
        "# except ModuleNotFoundError:\n",
        "#     !pip install -r \"ch4-requirements.txt\"\n",
        "\n",
        "# ==========================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5S86uIWZKVO"
      },
      "source": [
        "We will be building an LSTM model with an embedding layer trained on the fly. We will be following all the preprocessing steps as in the [DeepNN_Example.ipynb](https://github.com/practical-nlp/practical-nlp/blob/master/Ch4/DeepNN_Example.ipynb) notebook in this repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UsCn1xlo_MMX"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "#helper functions to lead the data\n",
        "def load_directory_data(directory):\n",
        "    data = {}\n",
        "    data[\"sentence\"] = []\n",
        "    data[\"sentiment\"] = []\n",
        "    for file_path in os.listdir(directory):\n",
        "        with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "            data[\"sentence\"].append(f.read())\n",
        "            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
        "    return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(directory):\n",
        "    pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "    neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "    pos_df[\"polarity\"] = 1\n",
        "    neg_df[\"polarity\"] = 0\n",
        "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "def download_and_load_datasets(force_download=False):\n",
        "    dataset = tf.keras.utils.get_file(\n",
        "      fname=\"aclImdb.tar.gz\", \n",
        "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "      extract=True)\n",
        "\n",
        "    train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                       \"aclImdb\", \"train\"))\n",
        "    test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                      \"aclImdb\", \"test\"))\n",
        "  \n",
        "    return train_df, test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dlBTwerPX1EV"
      },
      "outputs": [],
      "source": [
        "# try:\n",
        "#     from google.colab import files\n",
        "    \n",
        "#     if not os.path.exists('aclImdb'):\n",
        "#         train,test = download_and_load_datasets()\n",
        "#     else:\n",
        "#         train = load_dataset('aclImdb/train')\n",
        "#         test = load_dataset('aclImdb/test')\n",
        "    \n",
        "# except ModuleNotFoundError:\n",
        "#     # if not os.path.exists('data/bigdata/aclImdb'):\n",
        "#     #     train,test = download_and_load_datasets()\n",
        "#     if False:\n",
        "#         train,test = download_and_load_datasets()\n",
        "#     else:\n",
        "#         train = load_dataset('data/bigdata/aclImdb/train')\n",
        "#         test = load_dataset('Ddata/bigdata/aclImdb/test')\n",
        "\n",
        "\n",
        "train = load_dataset('../data/bigdata/aclImdb/train')\n",
        "test = load_dataset('../data/bigdata/aclImdb/test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hEpQWHnF-hOX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# MAX_SEQUENCE_LENGTH = 1000\n",
        "# MAX_NUM_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "MAX_NUM_WORDS = 200 \n",
        "EMBEDDING_DIM = 100 \n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "# vocab_size = 20000  # Max number of different word, i.e. model input dimension\n",
        "# maxlen = 1000 # Max number of words kept at the end of each text\n",
        "vocab_size = 200  # Max number of different word, i.e. model input dimension\n",
        "maxlen = 20 # Max number of words kept at the end of each text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "830AVGaZBfnf",
        "outputId": "1a80608b-b295-41c3-dc2f-bfbd701667d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['sentence', 'sentiment', 'polarity'], dtype='object')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "test = test[:1000]\n",
        "train = train[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FJo_FLISBhx6"
      },
      "outputs": [],
      "source": [
        "train_texts = train['sentence'].values\n",
        "train_labels = train['polarity'].values\n",
        "test_texts = test['sentence'].values\n",
        "# test_labels = test['polarity'].values\n",
        "\n",
        "labels_index = {'pos':1, 'neg':0} "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ1G3DH3dDQ4",
        "outputId": "6aee306f-91e4-45a3-fae1-db00d530ff2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['sentence', 'sentiment', 'polarity'], dtype='object')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8I6QgXldKd0",
        "outputId": "e96585e8-3a61-45f1-8873-247b4b6afa13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
              "       0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
              "       1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
              "       0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
              "       0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
              "       1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
              "       0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
              "       0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
              "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
              "       1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
              "       1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
              "       0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
              "       0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
              "       1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
              "       1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
              "       1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
              "       1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
              "       0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
              "       0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
              "       0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
              "       1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
              "       0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
              "       0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
              "       0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
              "       1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
              "       1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
              "       0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
              "       1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
              "       1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
              "       1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
              "       1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
              "       0, 0, 1, 0, 1, 1, 1, 0, 0, 1], dtype=int64)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_labels = test['polarity'].values\n",
        "test_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfPAZEEKajly"
      },
      "source": [
        "We need to design an sklearn pipeline with our model.\n",
        "What is a pipeline?<br>\n",
        "\n",
        "**Transformer** in scikit-learn - some class that have fit and transform method, or fit_transform method.\n",
        "\n",
        "**Predictor** - some class that has fit and predict methods, or fit_predict method.\n",
        "\n",
        "**Pipeline** is just an abstract notion, it's not some existing ml algorithm. Often in ML tasks you need to perform sequence of different transformations (find set of features, generate new features, select only some good features) of raw dataset before applying final estimator. Pipeline gives you a single interface for all 3 steps of transformation and resulting estimator. It encapsulates transformers and predictors inside"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train\n",
        "# print([len(x) for x in train])\n",
        "# print(train.shape)\n",
        "# print(test.shape)\n",
        "# print([x for x in test])\n",
        "# print(test['polarity'])\n",
        "# print(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ibekacAMMTsr"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.pipeline import TransformerMixin\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "\n",
        "class TextsToSequences(Tokenizer, BaseEstimator, TransformerMixin):\n",
        "    \"\"\" Sklearn transformer to convert texts to indices list \n",
        "    (e.g. [[\"the cute cat\"], [\"the dog\"]] -> [[1, 2, 3], [1, 4]])\"\"\"\n",
        "    def __init__(self,  **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "    def fit(self, texts, y=None):\n",
        "        self.fit_on_texts(texts)\n",
        "        return self\n",
        "    \n",
        "    def transform(self, texts, y=None):\n",
        "        temp_texts = self.texts_to_sequences(texts)\n",
        "        # print(temp_texts)\n",
        "        return temp_texts\n",
        "        # return np.array(temp_texts)\n",
        "        \n",
        "sequencer = TextsToSequences(num_words=vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Padder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\" Pad and crop uneven lists to the same length. \n",
        "    Only the end of lists longernthan the maxlen attribute are\n",
        "    kept, and lists shorter than maxlen are left-padded with zeros\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    maxlen: int\n",
        "        sizes of sequences after padding\n",
        "    max_index: int\n",
        "        maximum index known by the Padder, if a higher index is met during \n",
        "        transform it is transformed to a 0\n",
        "    \"\"\"\n",
        "    def __init__(self, maxlen=500):\n",
        "        self.maxlen = maxlen\n",
        "        self.max_index = None\n",
        "        \n",
        "    def fit(self, X, y=None):\n",
        "        self.max_index = pad_sequences(X, maxlen=self.maxlen).max()\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "        X = pad_sequences(X, maxlen=self.maxlen)\n",
        "        X[X > self.max_index] = 0\n",
        "        return X\n",
        "\n",
        "padder = Padder(maxlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mtEN5BjbyKM"
      },
      "source": [
        "We will only train for 2 epochs. A better model could be trained with more epochs and early stopping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XvXEYGBMWlW",
        "outputId": "4ea00082-b2b0-4760-b083-77b033daa59b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 3s 22ms/step - loss: 0.6926 - accuracy: 0.5010\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;textstosequences&#x27;, TextsToSequences()),\n",
              "                (&#x27;padder&#x27;, Padder(maxlen=20)),\n",
              "                (&#x27;sequential&#x27;,\n",
              "                 &lt;keras.src.engine.sequential.Sequential object at 0x0000019C62E1D450&gt;)])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;textstosequences&#x27;, TextsToSequences()),\n",
              "                (&#x27;padder&#x27;, Padder(maxlen=20)),\n",
              "                (&#x27;sequential&#x27;,\n",
              "                 &lt;keras.src.engine.sequential.Sequential object at 0x0000019C62E1D450&gt;)])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TextsToSequences</label><div class=\"sk-toggleable__content\"><pre>TextsToSequences()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Padder</label><div class=\"sk-toggleable__content\"><pre>Padder(maxlen=20)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Sequential</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.src.engine.sequential.Sequential object at 0x0000019C62E1D450&gt;</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('textstosequences', TextsToSequences()),\n",
              "                ('padder', Padder(maxlen=20)),\n",
              "                ('sequential',\n",
              "                 <keras.src.engine.sequential.Sequential object at 0x0000019C62E1D450>)])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM\n",
        "# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "batch_size = 64\n",
        "max_features = vocab_size + 1\n",
        "\n",
        "#Training an LSTM with embedding on the fly\n",
        "def create_model(max_features):\n",
        "    \"\"\" Model creation function: returns a compiled LSTM\"\"\"\n",
        "\n",
        "\n",
        "    rnnmodel = Sequential()\n",
        "    rnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
        "    rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    rnnmodel.add(Dense(1, activation='sigmoid'))\n",
        "    rnnmodel.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "    return rnnmodel\n",
        "\n",
        "\n",
        "# Use Keras Scikit-learn wrapper to instantiate a LSTM with all methods\n",
        "# required by Scikit-learn for the last step of a Pipeline\n",
        "sklearn_lstm = create_model(0)\n",
        "# sklearn_lstm = KerasClassifier(build_fn=create_model, epochs=2, batch_size=32, \n",
        "#                                max_features=max_features, verbose=1)\n",
        "\n",
        "# Build the Scikit-learn pipeline\n",
        "pipeline = make_pipeline(sequencer, padder, sklearn_lstm)\n",
        "\n",
        "# print(train_labels.shape)\n",
        "# print(train_texts.shape)\n",
        "# train_texts2 = [t[:10] for t in train_texts]\n",
        "# print(train_labels)\n",
        "pipeline.fit(train_texts, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTph3cZ2anzx",
        "outputId": "d469ca0d-836c-4911-a194-3f9b01253b94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 5ms/step\n"
          ]
        }
      ],
      "source": [
        "y_preds = pipeline.predict(test_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KKnIf5BcnQn",
        "outputId": "5f7f975d-d7da-4618-f63d-8f9ac3162c97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 52.90 %\n"
          ]
        }
      ],
      "source": [
        "y_preds2 = [y[0] for y in y_preds]\n",
        "y_preds2 = [1 if y > 0.5 else 0 for y in y_preds2]\n",
        "print('Test accuracy: {:.2f} %'.format(100*accuracy_score(y_preds2, test_labels)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "jd9mzgXrZ9ys",
        "outputId": "28ee9c4e-f38f-4b42-e7b2-b032230892b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 11: last 1000 words (only part used by the model)\n",
            "--------------------------------------------------\n",
            "I have a question for the writers and producers of \"Prozac Nation\": What is the root cause and what is the solution to the widespread problem of personal depression in America? In the moving performance of Christina Ricci as Liz Wurtzel, the film portrays a young woman with unlimited potential as a Harvard student and as a writer. But this is not a story of success, only one of self-destruction as we watch Liz bring misery into the lives everyone who comes in contact with her. The film examines divorce, family dysfunction, drugs, alcohol, and prescription medication as possible reasons for Liz's unhappiness. But none of those superficial explanations are satisfactory.<br /><br />At some point in the film, it would have been helpful to suggest that Liz needs to take responsibility for her life and her problems. No light was shed on what the film alleged to be a runaway problem in \"The United States of Depression.\" In the story, Liz had a caring therapist (Anne Heche), a caring roommate (Michele Williams), a caring boyfriend (Jason Biggs), and a troubled but caring parent (Jessica Lange). In a key scene in the film, Liz is lying in a hospital bed watching the break-up of the space shuttle Challenger. Instead of equating Challenger with Liz's life, the film should have used the image as a starting point for her healing and recovery.<br /><br />This film reminded me of a generic made-for-cable \"victim\" film on the Lifetime network. An excellent cast was wasted, especially in the earnest performance of Christina Ricci. The real-life Elizabeth Wurtzel obviously found within herself the resources to cope with her depression and become a successful author. It is unfortunate that the film could not offer us even the slightest glimpse into her courageous spirit.\n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Probability(positive) = [[0.5200008]]\n",
            "True class: negative\n",
            "157/157 [==============================] - 1s 5ms/step\n",
            "{0: [(62, 0.010000046866475784), (174, -0.0088929266456306), (5, 0.00839408079561151), (45, -0.007440038674542551), (91, -0.0050261187141991594), (171, -0.004581950028789771), (56, -0.003980775168899761), (7, 0.0018065860498359518), (33, 0.0015804130146455716), (173, 0.0011764055762746803)]}\n"
          ]
        }
      ],
      "source": [
        "# We choose a sample from test set\n",
        "idx = 11\n",
        "text_sample = test_texts[idx]\n",
        "class_names = ['negative', 'positive']\n",
        "\n",
        "print('Sample {}: last 1000 words (only part used by the model)'.format(idx))\n",
        "print('-'*50)\n",
        "print(\" \".join(text_sample.split()[-1000:]))\n",
        "print('-'*50)\n",
        "print('Probability(positive) =', pipeline.predict([text_sample]))\n",
        "print('True class: %s' % class_names[test_labels[idx]])\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from collections import OrderedDict\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "explainer = LimeTextExplainer(class_names=class_names)\n",
        "# print(text_sample)\n",
        "explanation = explainer.explain_instance(text_sample, pipeline.predict, num_features=10, labels=(0,))\n",
        "print(explanation.local_exp)\n",
        "weights = OrderedDict(explanation.as_list(label=0))\n",
        "lime_weights = pd.DataFrame({'words': list(weights.keys()), 'weights': list(weights.values())})\n",
        "\n",
        "sns.barplot(x=\"words\", y=\"weights\", data=lime_weights)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('Sample {} features weights given by LIME'.format(idx))\n",
        "plt.savefig('../pngs/lime_weights.png', bbox_inches='tight')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tdpfNvHcaDB"
      },
      "source": [
        "We have used the LIME interpretation to provide explanations for a recurrent neural network. Looking at the graph we understand that the sentence is negative and the word \"worst\" affects it the most.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "09_Lime_RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
