{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will demonstrate how you can use Tensorboard to visualize word embeddings which we created in the Training_embeddings_using_gensim.ipynb notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install only the requirements of this notebook, uncomment the lines below and run this cell\n",
    "\n",
    "# ===========================\n",
    "\n",
    "# !pip install tensorflow==1.14.0\n",
    "# !pip install gensim==3.6.0\n",
    "# !pip install numpy==1.19.5\n",
    "\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install the requirements for the entire chapter, uncomment the lines below and run this cell\n",
    "\n",
    "# ===========================\n",
    "\n",
    "# try :\n",
    "#     import google.colab\n",
    "#     !curl https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch3/ch3-requirements.txt | xargs -n 1 -L 1 pip install\n",
    "# except ModuleNotFoundError :\n",
    "#     !pip install -r \"ch3-requirements.txt\"\n",
    "\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the required imports\n",
    "import warnings #ignoring the generated warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "# from tensorflow.contrib.tensorboard.plugins import projector\n",
    "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model\n",
    "# cwd=os.getcwd() \n",
    "model = KeyedVectors.load_word2vec_format(\"../models/word2vec_cbow.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the model's vocabulary size\n",
    "max_size = len(model.index_to_key)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a numpy array of 0s with the size of the vocabulary and dimensions of our model\n",
    "w2v = np.zeros((max_size,model.vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we create a new file called metadata.tsv where we save all the words in our model \n",
    "#we also store the embedding of each word in the w2v matrix\n",
    "if not os.path.exists('../output/projections'):\n",
    "    os.makedirs('../output/projections')\n",
    "    \n",
    "with open(\"../output/projections/metadata.tsv\", 'w+',encoding=\"utf-8\") as file_metadata: #changed    added encoding=\"utf-8\"\n",
    "    \n",
    "    for i, word in enumerate(model.index_to_key[:max_size]):\n",
    "        \n",
    "        #store the embeddings of the word\n",
    "        w2v[i] = model[word]\n",
    "        \n",
    "        #write the word to a file \n",
    "        file_metadata.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2v)\n",
    "logdir = \"../output/logs/\"\n",
    "weights = model.vectors\n",
    "\n",
    "# Create a TensorFlow variable to store the weights.\n",
    "weights_var = tf.Variable(weights, name='weights')\n",
    "\n",
    "# Save the weights to a TensorFlow checkpoint.\n",
    "# tf.train.Saver().save(weights_var, '../output/logs/weights.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.2186 - accuracy: 0.9352 - val_loss: 0.0991 - val_accuracy: 0.9695\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0959 - accuracy: 0.9703 - val_loss: 0.0756 - val_accuracy: 0.9759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1c1a4e1a4d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most of the above code doesnt work anymore with new versions of tensorboard\n",
    "# this method should work to show how to use tensorboard\n",
    "\n",
    "import datetime\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()[:100]\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "def create_model():\n",
    "  return tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28), name='layers_flatten'),\n",
    "    tf.keras.layers.Dense(512, activation='relu', name='layers_dense'),\n",
    "    tf.keras.layers.Dropout(0.2, name='layers_dropout'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax', name='layers_dense_2')\n",
    "  ])\n",
    "\n",
    "model = create_model()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# log_dir = \"../ouptput/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"../output/logs/fit/\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(x=x_train, \n",
    "          y=y_train, \n",
    "          epochs=2, \n",
    "          validation_data=(x_test, y_test), \n",
    "          callbacks=[tensorboard_callback])\n",
    "\n",
    "# %tensorboard --logdir ../output/logs/fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Pretty much everything below has been deprecated\n",
    "#initializing tf session\n",
    "# sess = tf.InteractiveSession()\n",
    "# sess = tf.compat.v1.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the tensorflow variable called embeddings that holds the word embeddings:\n",
    "# with tf.device(\"/cpu:0\"):\n",
    "#     embedding = tf.Variable(w2v, trainable=False, name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize all variables\n",
    "# automatically done now\n",
    "# tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#object of the saver class which is actually used for saving and restoring variables to and from our checkpoints\n",
    "# saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with FileWriter,we save summary and events to the event file\n",
    "# writer = tf.summary.FileWriter('projections', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the projectors and add the embeddings\n",
    "# config = projector.ProjectorConfig()\n",
    "# embed= config.embeddings.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify our tensor_name as embedding and metadata_path to the metadata.tsv file\n",
    "# embed.tensor_name = 'embedding'\n",
    "# embed.metadata_path = 'metadata.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "# projector.visualize_embeddings(writer, config)\n",
    "\n",
    "# saver.save(sess, 'projections/model.ckpt', global_step=max_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a terminal window and type the following command\n",
    "\n",
    "tensorboard --logdir=projections --port=8000\n",
    "\n",
    "If the tensorboard does not work for you try providing the absolute path for projections and re-run the above command\n",
    "\n",
    "If youve done everything right until you will get a link in your terminal through which you can access the tensorboard. Click on the link or copy paste it in your browser. You should see something similar to this.\n",
    "![TensorBoard-1](Images/TensorBoard-1.png)\n",
    "<br>\n",
    "In the top right corner near \"INACTIVE\" click the dropdown arrow. And select PROJECTIONS from te dropdown menu\n",
    "![TensorBoard-2](Images/TensorBoard-2.png)\n",
    "<br>\n",
    "Wait for a few seconds for it to load. You can now see your embeddings there are a lot of setting you can play around and experiment with.\n",
    "![TensorBoard-3](Images/TensorBoard-3.png)\n",
    "<br>\n",
    "Output when we search for a specific word in this case \"human\" and isolate only those points\n",
    "![TensorBoard-4](Images/TensorBoard-4.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
