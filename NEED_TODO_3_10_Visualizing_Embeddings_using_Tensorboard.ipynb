{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will demonstrate how you can use Tensorboard to visualize word embeddings which we created in the Training_embeddings_using_gensim.ipynb notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install only the requirements of this notebook, uncomment the lines below and run this cell\n",
    "\n",
    "# ===========================\n",
    "\n",
    "# !pip install tensorflow==1.14.0\n",
    "# !pip install gensim==3.6.0\n",
    "# !pip install numpy==1.19.5\n",
    "\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install the requirements for the entire chapter, uncomment the lines below and run this cell\n",
    "\n",
    "# ===========================\n",
    "\n",
    "# try :\n",
    "#     import google.colab\n",
    "#     !curl https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch3/ch3-requirements.txt | xargs -n 1 -L 1 pip install\n",
    "# except ModuleNotFoundError :\n",
    "#     !pip install -r \"ch3-requirements.txt\"\n",
    "\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the required imports\n",
    "import warnings #ignoring the generated warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "# from tensorflow.contrib.tensorboard.plugins import projector\n",
    "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model\n",
    "# cwd=os.getcwd() \n",
    "model = KeyedVectors.load_word2vec_format(\"data/word2vec_cbow.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the model's vocabulary size\n",
    "max_size = len(model.index_to_key)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a numpy array of 0s with the size of the vocabulary and dimensions of our model\n",
    "w2v = np.zeros((max_size,model.vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we create a new file called metadata.tsv where we save all the words in our model \n",
    "#we also store the embedding of each word in the w2v matrix\n",
    "if not os.path.exists('output/projections'):\n",
    "    os.makedirs('output/projections')\n",
    "    \n",
    "with open(\"output/projections/metadata.tsv\", 'w+',encoding=\"utf-8\") as file_metadata: #changed    added encoding=\"utf-8\"\n",
    "    \n",
    "    for i, word in enumerate(model.index_to_key[:max_size]):\n",
    "        \n",
    "        #store the embeddings of the word\n",
    "        w2v[i] = model[word]\n",
    "        \n",
    "        #write the word to a file \n",
    "        file_metadata.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing tf session\n",
    "# sess = tf.InteractiveSession()\n",
    "sess = tf.compat.v1.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the tensorflow variable called embeddings that holds the word embeddings:\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    embedding = tf.Variable(w2v, trainable=False, name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize all variables\n",
    "# automatically done now\n",
    "# tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.compat.v2.summary.experimental' has no attribute 'embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#object of the saver class which is actually used for saving and restoring variables to and from our checkpoints\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Deprecated\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# saver = tf.train.Saver()\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m sess:\n\u001b[1;32m----> 7\u001b[0m     tf\u001b[39m.\u001b[39;49msummary\u001b[39m.\u001b[39;49mexperimental\u001b[39m.\u001b[39;49membedding(embedding, model\u001b[39m.\u001b[39mindex_to_key[:max_size], name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmy_embeddings\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m     writer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mFileWriter(\u001b[39m\"\u001b[39m\u001b[39m./output\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m     writer\u001b[39m.\u001b[39madd_summary(sess\u001b[39m.\u001b[39mrun(tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mall_v2_summary_ops()))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.compat.v2.summary.experimental' has no attribute 'embedding'"
     ]
    }
   ],
   "source": [
    "#object of the saver class which is actually used for saving and restoring variables to and from our checkpoints\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorboard.summary._tf.summary' has no attribute 'FileWriter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#with FileWriter,we save summary and events to the event file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m writer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49msummary\u001b[39m.\u001b[39;49mFileWriter(\u001b[39m'\u001b[39m\u001b[39mprojections\u001b[39m\u001b[39m'\u001b[39m, sess\u001b[39m.\u001b[39mgraph)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorboard.summary._tf.summary' has no attribute 'FileWriter'"
     ]
    }
   ],
   "source": [
    "#with FileWriter,we save summary and events to the event file\n",
    "writer = tf.summary.FileWriter('projections', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the projectors and add the embeddings\n",
    "config = projector.ProjectorConfig()\n",
    "embed= config.embeddings.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify our tensor_name as embedding and metadata_path to the metadata.tsv file\n",
    "embed.tensor_name = 'embedding'\n",
    "embed.metadata_path = 'metadata.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projections/model.ckpt-161017'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the model\n",
    "projector.visualize_embeddings(writer, config)\n",
    "\n",
    "saver.save(sess, 'projections/model.ckpt', global_step=max_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a terminal window and type the following command\n",
    "\n",
    "tensorboard --logdir=projections --port=8000\n",
    "\n",
    "If the tensorboard does not work for you try providing the absolute path for projections and re-run the above command\n",
    "\n",
    "If youve done everything right until you will get a link in your terminal through which you can access the tensorboard. Click on the link or copy paste it in your browser. You should see something similar to this.\n",
    "![TensorBoard-1](Images/TensorBoard-1.png)\n",
    "<br>\n",
    "In the top right corner near \"INACTIVE\" click the dropdown arrow. And select PROJECTIONS from te dropdown menu\n",
    "![TensorBoard-2](Images/TensorBoard-2.png)\n",
    "<br>\n",
    "Wait for a few seconds for it to load. You can now see your embeddings there are a lot of setting you can play around and experiment with.\n",
    "![TensorBoard-3](Images/TensorBoard-3.png)\n",
    "<br>\n",
    "Output when we search for a specific word in this case \"human\" and isolate only those points\n",
    "![TensorBoard-4](Images/TensorBoard-4.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
