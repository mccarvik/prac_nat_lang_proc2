{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFcnXxiQX-bS"
   },
   "source": [
    "In this notebook we demonstrate how we can leverage BERT to perform NER on conll2003 dataset.<br>\n",
    "This notebook requires a GPU to get setup. We suggest you to run this on your local machine only if you have a GPU setup or else you can use google colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZNKhShOaylVt",
    "outputId": "8c06a549-62a4-42dc-df3f-d727d625c557"
   },
   "outputs": [],
   "source": [
    "#Installing required packages\n",
    "# try :\n",
    "#     from google.colab import files\n",
    "#     %tensorflow_version 1.x\n",
    "    \n",
    "# except ModuleNotFoundError :\n",
    "#     Print(\"Not Using Colab\")\n",
    "    \n",
    "# !pip install pytorch-pretrained-bert==0.4.0\n",
    "# !pip install seqeval==0.0.12\n",
    "\n",
    "#importing packages for string processing,dataframe handling, array manipulations, etc\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#importing all the pytorch packages\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam\n",
    "\n",
    "#importing additonal packages to aid preprocessing of data\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#importing packages to calculate the f1_score of our model\n",
    "from seqeval.metrics import f1_score, recall_score, precision_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "hVfxrl-HOkTn",
    "outputId": "a4770c6a-b778-4233-a733-76f0326eef4e"
   },
   "outputs": [],
   "source": [
    "# #uploading data into google colab\n",
    "# #upload the test.txt and train.txt files respectively\n",
    "# try :\n",
    "#     from google.colab import files\n",
    "#     uploaded = files.upload()\n",
    "\n",
    "# except ModuleNotFoundError :\n",
    "#     print(\"Not using Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "HkbURKk5y0_1"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the training/testing data. \n",
    "input: conll format data, but with only 2 tab separated colums - words and NEtags.\n",
    "output: A list where each item is 2 lists.  sentence as a list of tokens, NER tags as a list for each token.\n",
    "\"\"\"\n",
    "#functions for preparing the data in the *.txt files\n",
    "def load__data_conll(file_path):\n",
    "    myoutput,words,tags = [],[],[]\n",
    "    fh = open(file_path)\n",
    "    for line in fh:\n",
    "        line = line.strip()\n",
    "        if \"\\t\" not in line:\n",
    "            #Sentence ended.\n",
    "            myoutput.append([words,tags])\n",
    "            words,tags = [],[]\n",
    "        else:\n",
    "            word, tag = line.split(\"\\t\")\n",
    "            words.append(word)\n",
    "            tags.append(tag)\n",
    "    fh.close()\n",
    "    return myoutput\n",
    "\n",
    "\"\"\"\n",
    "Get features for all words in the sentence\n",
    "Features:\n",
    "- word context: a window of 2 words on either side of the current word, and current word.\n",
    "- POS context: a window of 2 POS tags on either side of the current word, and current tag. \n",
    "input: sentence as a list of tokens.\n",
    "output: list of dictionaries. each dict represents features for that word.\n",
    "\"\"\"\n",
    "def sent2feats(sentence):\n",
    "    feats = []\n",
    "    sen_tags = pos_tag(sentence) #This format is specific to this POS tagger!\n",
    "    for i in range(0,len(sentence)):\n",
    "        word = sentence[i]\n",
    "        wordfeats = {}\n",
    "       #word features: word, prev 2 words, next 2 words in the sentence.\n",
    "        wordfeats['word'] = word\n",
    "        if i == 0:\n",
    "            wordfeats[\"prevWord\"] = wordfeats[\"prevSecondWord\"] = \"<S>\"\n",
    "        elif i==1:\n",
    "            wordfeats[\"prevWord\"] = sentence[0]\n",
    "            wordfeats[\"prevSecondWord\"] = \"</S>\"\n",
    "        else:\n",
    "            wordfeats[\"prevWord\"] = sentence[i-1]\n",
    "            wordfeats[\"prevSecondWord\"] = sentence[i-2]\n",
    "        #next two words as features\n",
    "        if i == len(sentence)-2:\n",
    "            wordfeats[\"nextWord\"] = sentence[i+1]\n",
    "            wordfeats[\"nextNextWord\"] = \"</S>\"\n",
    "        elif i==len(sentence)-1:\n",
    "            wordfeats[\"nextWord\"] = \"</S>\"\n",
    "            wordfeats[\"nextNextWord\"] = \"</S>\"\n",
    "        else:\n",
    "            wordfeats[\"nextWord\"] = sentence[i+1]\n",
    "            wordfeats[\"nextNextWord\"] = sentence[i+2]\n",
    "        \n",
    "        #POS tag features: current tag, previous and next 2 tags.\n",
    "        wordfeats['tag'] = sen_tags[i][1]\n",
    "        if i == 0:\n",
    "            wordfeats[\"prevTag\"] = wordfeats[\"prevSecondTag\"] = \"<S>\"\n",
    "        elif i == 1:\n",
    "            wordfeats[\"prevTag\"] = sen_tags[0][1]\n",
    "            wordfeats[\"prevSecondTag\"] = \"</S>\"\n",
    "        else:\n",
    "            wordfeats[\"prevTag\"] = sen_tags[i - 1][1]\n",
    "\n",
    "            wordfeats[\"prevSecondTag\"] = sen_tags[i - 2][1]\n",
    "            # next two words as features\n",
    "        if i == len(sentence) - 2:\n",
    "            wordfeats[\"nextTag\"] = sen_tags[i + 1][1]\n",
    "            wordfeats[\"nextNextTag\"] = \"</S>\"\n",
    "        elif i == len(sentence) - 1:\n",
    "            wordfeats[\"nextTag\"] = \"</S>\"\n",
    "            wordfeats[\"nextNextTag\"] = \"</S>\"\n",
    "        else:\n",
    "            wordfeats[\"nextTag\"] = sen_tags[i + 1][1]\n",
    "            wordfeats[\"nextNextTag\"] = sen_tags[i + 2][1]\n",
    "        #That is it! You can add whatever you want!\n",
    "        feats.append(wordfeats)\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "uuURLq19f5fz"
   },
   "outputs": [],
   "source": [
    "#preprocess the data by calling the functions\n",
    "# try :\n",
    "#     from google.colab import files\n",
    "#     train_path = 'train.txt'\n",
    "#     test_path = 'test.txt' \n",
    "\n",
    "# except ModuleNotFoundError :\n",
    "#     train_path = 'Data/conlldata/train.txt'\n",
    "#     test_path = 'Data/conlldata/test.txt'\n",
    "train_path = 'Data/conlldata/train.txt'\n",
    "test_path = 'Data/conlldata/test3.txt'\n",
    "    \n",
    "conll_train = load__data_conll(train_path)\n",
    "conll_test = load__data_conll(test_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oYm4B66pBN6T",
    "outputId": "ba0447dc-fcf2-4a3d-93d1-a54de1d3686e"
   },
   "outputs": [],
   "source": [
    "#BERT needs us to pre-process the data in a particular way.\n",
    "#Lets take the raw data from the txt files\n",
    "df_train = pd.read_csv(train_path, engine=\"python\",delimiter=\"\\t\",header=None,encoding='utf-8', on_bad_lines =\"skip\")\n",
    "df_test = pd.read_csv(test_path, engine=\"python\",delimiter=\"\\t\",encoding='utf-8',header=None, on_bad_lines =\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "tQS3WdwLBOJe"
   },
   "outputs": [],
   "source": [
    "#merge \n",
    "df = pd.merge(df_train,df_test)\n",
    "label = list(df[1].values)#we will be using this to make a set of all unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "av7yRSvrX4-z",
    "outputId": "e400cdc3-b2f6-4168-a1ba-556d50b1c4a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14041, 2)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(conll_train)\n",
    "len(conll_train) #calculating the length\n",
    "np.array(conll_train, dtype=object).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3BGxbUspYAPU",
    "outputId": "bbff2664-9f50-4464-bfe4-ec4a1b2fcc8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3453, 2)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(conll_test, dtype=object).shape#calculating the size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBA0ccq2YUGN"
   },
   "source": [
    "We need to join all the tokens into a single sentence. We will use the untokenize function in token_utils from [this](https://github.com/commonsense/metanl/blob/master/metanl/token_utils.py) github repo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "3046zKhaTXOW"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def untokenize(words):\n",
    "    \"\"\"\n",
    "    Untokenizing a text undoes the tokenizing operation, restoring\n",
    "    punctuation and spaces to the places that people expect them to be.\n",
    "    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n",
    "    except for line breaks.\n",
    "    \"\"\"\n",
    "    text = ' '.join(words)\n",
    "    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n",
    "    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
    "    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
    "    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
    "         \"can not\", \"cannot\")\n",
    "    step6 = step5.replace(\" ` \", \" '\")\n",
    "    return step6.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvtQJ1Fxa89j"
   },
   "source": [
    "Lets start with pre-processing the data for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "uEM8-XuPYeWt"
   },
   "outputs": [],
   "source": [
    "#lets convert them to dataframs for easier handling\n",
    "df_train = pd.DataFrame(conll_train,columns=[\"sentence\",\"labels\"])\n",
    "df_test = pd.DataFrame(conll_test,columns=[\"sentence\",\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5WgHCbX_ZAqG",
    "outputId": "f07ba787-8de7-4896-c9e3-cc15806503d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of sentences: 17494\n",
      "No of labels: 17494\n"
     ]
    }
   ],
   "source": [
    "#getting all the sentences and labels present in both test and train\n",
    "sentences = list(df_train['sentence'])+list(df_test['sentence'])\n",
    "print(\"No of sentences:\",len(sentences))\n",
    "labels = list(df_train['labels'])+list(df_test['labels']) \n",
    "print(\"No of labels:\",len(labels))\n",
    "\n",
    "sentences = sentences[:1000]\n",
    "labels = labels[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "_KW3zKy7ZAnB",
    "outputId": "4f63698f-b803-4f3a-e07c-61c8c9d60174"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EU rejects German call to boycott British lamb.'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [untokenize(sent) for sent in sentences]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W88K8YVXh7QA"
   },
   "source": [
    "We need to now tokenize the sentences and then add the CLS and SEP tokens as BERT expects the input in such a format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3JUPhdRFzea1",
    "outputId": "ecf25f59-5254-4fe0-be36-9ab4e7678b76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#setting up pytorch to use GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "#prescribed configurations that we need to fix for BERT.\n",
    "MAX_LEN = 75\n",
    "bs = 32\n",
    "\n",
    "#BERT's implementation comes with a pretained tokenizer and a defined vocabulary\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "#tokenizing the text \n",
    "tokenized_texts = list(map(lambda x: ['[CLS]'] + tokenizer.tokenize(x) + ['[SEP]'] , sentences))\n",
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Niy1e_tU7F0c",
    "outputId": "69913e29-6997-477e-a105-60008eea2ba6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "7xxx7mF28NZi"
   },
   "outputs": [],
   "source": [
    "#pre-processing the labels\n",
    "#converting tags to indices \n",
    "tags_vals = list(set(label))  \n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "mvUnUjvk8sOB"
   },
   "outputs": [],
   "source": [
    "#We now need to give BERT input ids,ie, a sequence of integers which uniquely identify each input token to its index number.\n",
    "#cutting and padding the tokens and labels to our desired length\n",
    "\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "tDb-q3HXEumu"
   },
   "outputs": [],
   "source": [
    "#BERT supports something called attention masks\n",
    "#Tells the model which tokens should be attended to, and which should not.\n",
    "#learn more about this at https://huggingface.co/transformers/glossary.html#attention-mask\n",
    "\n",
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drV54DRsbYPc"
   },
   "source": [
    "Now we need to split the data into train and validation. Convert it to tensors and then create an iterator for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "_LuJ9LeDEwex"
   },
   "outputs": [],
   "source": [
    "#split the dataset to use 20% to validate the model.\n",
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n",
    "                                                            random_state=2020, test_size=0.2)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2020, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "KuS8WeQyEyG6"
   },
   "outputs": [],
   "source": [
    "#pytorch requires inputs to be in the form of torch tensors\n",
    "#Learn more about torch tensors at https://pytorch.org/docs/stable/tensors.html\n",
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-YtGlTXEztA",
    "outputId": "17b852e0-f9eb-4d36-8488-1847ee3f25bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Loaders Ready\n",
      "Test Data Loaders Ready\n"
     ]
    }
   ],
   "source": [
    "#Define the Data Loaders\n",
    "#Shuffle the data at training time\n",
    "#Pass them sequentially during test time\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "print(\"Train Data Loaders Ready\")\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)\n",
    "print(\"Test Data Loaders Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ajuboku7E1Y0",
    "outputId": "0dc44313-ce6b-421c-e278-16a6f5bf8bb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model ready to use\n"
     ]
    }
   ],
   "source": [
    "# BertForTokenClassification class of pytorch-pretrained-bert package provides  for token-level predictions\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))#loading pre trained bert\n",
    "print(\"BERT model ready to use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "En7S5Jj2E3Sh",
    "outputId": "c85187b5-f19d-40f9-8a80-82a08be316c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForTokenClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): BertLayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Passing model parameters into GPU\n",
    "if torch.cuda.is_available():    \n",
    "    print(\"Passing Model parameters in GPU\")\n",
    "    print(model.cuda()) \n",
    "else: \n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dlWDBc1az7H"
   },
   "source": [
    "Finally, we move to fine  tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "it5l2g-cE439",
    "outputId": "2f494a2a-93ff-4414-87ee-275e5459b068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine Tuning BERT\n"
     ]
    }
   ],
   "source": [
    "#Before starting fine tuing we need to add the optimizer. Generally Adam is used\n",
    "#weight_decay is added as regularization to the main weight matrices\n",
    "print(\"Fine Tuning BERT\")\n",
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "w7jNHHs1E6u1"
   },
   "outputs": [],
   "source": [
    "#accuracy\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZHdynvRFDv1",
    "outputId": "3138d86b-6eae-4b00-b196-f6d5e307a8e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.9530907678604126\n",
      "Validation loss: 0.6174175824437823\n",
      "Validation Accuracy: 0.9648214285714286\n",
      "F1-Score: 0.0\n",
      "Recall Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 1/2 [08:42<08:42, 522.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score: 0.0\n",
      "Train loss: 0.5901277136802673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 2/2 [16:19<00:00, 489.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.51308103118624\n",
      "Validation Accuracy: 0.9689880952380953\n",
      "F1-Score: 0.21757322175732216\n",
      "Recall Score: 0.52\n",
      "Precision Score: 0.13756613756613756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Add the epoch number. The bert paper recomends 3-4\n",
    "epochs = 2\n",
    "max_grad_norm = 1.0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "train_loss_set=[]\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # forward pass\n",
    "        b_labels = b_labels.long()\n",
    "        loss = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_mask, labels=b_labels)\n",
    "        train_loss_set.append(loss)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    # VALIDATION on validation set\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            b_labels = b_labels.long()\n",
    "            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                                  attention_mask=b_input_mask, labels=b_labels)\n",
    "            logits = model(b_input_ids, token_type_ids=None,\n",
    "                           attention_mask=b_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss/nb_eval_steps\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "    pred_tags = [pred_tags]\n",
    "    valid_tags = [valid_tags]\n",
    "    print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
    "    print(\"Recall Score: {}\".format(recall_score(pred_tags, valid_tags)))\n",
    "    print(\"Precision Score: {}\".format(precision_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "cVjE3tA3dJe0",
    "outputId": "faf96f9f-96ad-4c69-a32a-bc6b7f2e478b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABNoAAAK9CAYAAADyqgGSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA7fklEQVR4nO3dfZiWdYH3/88AMajAiKAzjCKomJYGFA8Tqauuk+Caymp7I1s36tG9ta2a7mgmqeBTi5qV641KuXfSkw+5e2utaxiS2N2KUqiZT622KqgM+LDMACbYzPX7w5/TTqIifvECfL2O4zri+l7f87y+J8dxRse78zrPmkqlUgkAAAAA8I70qPYCAAAAAGBrILQBAAAAQAFCGwAAAAAUILQBAAAAQAFCGwAAAAAUILQBAAAAQAFCGwAAAAAUILQBAAAAQAFCGwAAAAAUILQBAGyhjj/++AwbNmyjtj333HNTU1NTdkEb6J2sGwBgcya0AQAUVlNTs0GvBQsWVHupAAAUVFOpVCrVXgQAwNbk+9//frf33/3udzNv3rx873vf6zb+8Y9/PPX19Rv9Pa+88ko6OztTW1v7trf9wx/+kD/84Q/p06fPRn//xjr++OOzYMGCPPnkk+/6dwMAbEq9qr0AAICtzac//elu7+++++7MmzfvdeN/6qWXXsq22267wd/zvve9b6PWlyS9evVKr17+pyAAQEl+OgoAUAUHHXRQ9t133yxevDh/9md/lm233TZf/vKXkyQ/+tGPcvjhh6exsTG1tbXZY489csEFF6Sjo6PbPv70XmdPPvlkampqcumll+Zb3/pW9thjj9TW1mbs2LH55S9/2W3b9d2jraamJieddFJuvvnm7Lvvvqmtrc0+++yTuXPnvm79CxYsyJgxY9KnT5/sscce+eY3v/mO7vu2Zs2anHbaaRkyZEhqa2uz11575dJLL82f/vhi3rx52X///bP99tunb9++2Wuvvbr+3l7zv//3/84+++yTbbfdNgMGDMiYMWNy7bXXbtS6AADeDv83JgBAlbzwwgs57LDDcuyxx+bTn/50189I58yZk759+6alpSV9+/bNz372s0yfPj3t7e356le/+pb7vfbaa7Nq1ap87nOfS01NTS655JIcffTR+c///M+3vAruF7/4Rf7v//2/+bu/+7v069cvl19+eY455pgsWbIkAwcOTJLcd999mThxYgYPHpzzzjsvHR0dOf/887Pjjjtu1N9DpVLJkUcemTvuuCOf+cxnMmrUqNx222354he/mGeeeSbf+MY3kiQPPfRQPvGJT2TEiBE5//zzU1tbm8cffzz//u//3rWvq6++Ol/4whfyyU9+MqecckpefvnlPPDAA7nnnnvy13/91xu1PgCADSW0AQBUSWtra2bPnp3Pfe5z3cavvfbabLPNNl3v//Zv/zZ/+7d/myuvvDIXXnjhW96TbcmSJXnssccyYMCAJMlee+2Vo446Krfddls+8YlPvOm2jzzySB5++OHsscceSZKDDz44I0eOzHXXXZeTTjopSTJjxoz07Nkz//7v/57GxsYkyf/4H/8jH/jAB97eX8D/78c//nF+9rOf5cILL8xZZ52VJDnxxBPzV3/1V/nHf/zHnHTSSdljjz0yb968rFu3Lj/5yU8yaNCg9e7r3/7t37LPPvvkxhtv3Ki1AAC8E346CgBQJbW1tTnhhBNeN/7fI9uqVavy/PPP54ADDshLL72URx999C33O3ny5K7IliQHHHBAkuQ///M/33Lb5ubmrsiWJCNGjEj//v27tu3o6Mjtt9+eSZMmdUW2JBk+fHgOO+ywt9z/+tx6663p2bNnvvCFL3QbP+2001KpVPKTn/wkSbL99tsnefWntZ2dnevd1/bbb5+nn376dT+VBQB4NwhtAABVsvPOO6d3796vG3/ooYfyl3/5l6mrq0v//v2z4447dj1Ioa2t7S33u+uuu3Z7/1p0+6//+q+3ve1r27+27YoVK/L73/8+w4cPf9289Y1tiKeeeiqNjY3p169ft/HXrpB76qmnkrwaEPfbb7/8r//1v1JfX59jjz02P/zhD7tFty996Uvp27dvxo0blz333DMnnnhit5+WAgBsSkIbAECV/Pcr116zcuXKHHjggfn1r3+d888/P//6r/+aefPm5eKLL06SN7yS67/r2bPnesf/9MECpbfd1LbZZpv8/Oc/z+23357/+T//Zx544IFMnjw5H//4x7seFPGBD3wgv/3tb3P99ddn//33z7/8y79k//33z4wZM6q8egDgvUBoAwDYjCxYsCAvvPBC5syZk1NOOSWf+MQn0tzc3O2noNW00047pU+fPnn88cdf99n6xjbE0KFD8+yzz2bVqlXdxl/7mezQoUO7xnr06JFDDjkkX//61/Pwww/nK1/5Sn72s5/ljjvu6Jqz3XbbZfLkybnmmmuyZMmSHH744fnKV76Sl19+eaPWBwCwoYQ2AIDNyGtXlP33K8jWrVuXK6+8slpL6qZnz55pbm7OzTffnGeffbZr/PHHH++6l9rb9Rd/8Rfp6OjIrFmzuo1/4xvfSE1NTde931588cXXbTtq1Kgkydq1a5O8+iTX/65379754Ac/mEqlkldeeWWj1gcAsKE8dRQAYDPysY99LAMGDMhxxx2XL3zhC6mpqcn3vve9zeKnm68599xz89Of/jT77bdfPv/5z3dFsn333Tf333//297fEUcckYMPPjhnnXVWnnzyyYwcOTI//elP86Mf/Sinnnpq18MZzj///Pz85z/P4YcfnqFDh2bFihW58sors8suu2T//fdPkhx66KFpaGjIfvvtl/r6+jzyyCOZNWtWDj/88NfdAw4AoDShDQBgMzJw4MDccsstOe2003L22WdnwIAB+fSnP51DDjkkEyZMqPbykiSjR4/OT37yk5x++uk555xzMmTIkJx//vl55JFHNuipqH+qR48e+fGPf5zp06fnhhtuyDXXXJNhw4blq1/9ak477bSueUceeWSefPLJfPvb387zzz+fQYMG5cADD8x5552Xurq6JMnnPve5/OAHP8jXv/71rF69Orvssku+8IUv5Oyzzy52/AAAb6Smsjn936MAAGyxJk2alIceeiiPPfZYtZcCAFAV7tEGAMDb9vvf/77b+8ceeyy33nprDjrooOosCABgM+CKNgAA3rbBgwfn+OOPz+67756nnnoqV111VdauXZv77rsve+65Z7WXBwBQFe7RBgDA2zZx4sRcd911aW1tTW1tbcaPH59/+Id/ENkAgPe0zeKno1dccUWGDRuWPn36pKmpKYsWLXrDuVdffXUOOOCADBgwIAMGDEhzc/Pr5h9//PGpqanp9po4ceKmPgwAgPeMa665Jk8++WRefvnltLW1Ze7cufnIRz5S7WUBAFRV1UPbDTfckJaWlsyYMSP33ntvRo4cmQkTJmTFihXrnb9gwYJMmTIld9xxRxYuXJghQ4bk0EMPzTPPPNNt3sSJE7Ns2bKu13XXXfduHA4AAAAA71FVv0dbU1NTxo4dm1mzZiVJOjs7M2TIkJx88sk588wz33L7jo6ODBgwILNmzcrUqVOTvHpF28qVK3PzzTdvyqUDAAAAQJeq3qNt3bp1Wbx4caZNm9Y11qNHjzQ3N2fhwoUbtI+XXnopr7zySnbYYYdu4wsWLMhOO+2UAQMG5M///M9z4YUXZuDAgevdx9q1a7N27dqu952dnXnxxRczcODA1NTUbMSRAQAAALC1qFQqWbVqVRobG9Ojxxv/QLSqoe35559PR0dH6uvru43X19fn0Ucf3aB9fOlLX0pjY2Oam5u7xiZOnJijjz46u+22W373u9/ly1/+cg477LAsXLgwPXv2fN0+Zs6cmfPOO++dHQwAAAAAW7WlS5dml112ecPPt+injl500UW5/vrrs2DBgvTp06dr/Nhjj+3684c+9KGMGDEie+yxRxYsWJBDDjnkdfuZNm1aWlpaut63tbVl1113zdKlS9O/f/9NexAAAAAAbNba29szZMiQ9OvX703nVTW0DRo0KD179szy5cu7jS9fvjwNDQ1vuu2ll16aiy66KLfffntGjBjxpnN33333DBo0KI8//vh6Q1ttbW1qa2tfN96/f3+hDQAAAIAkectbjFX1qaO9e/fO6NGjM3/+/K6xzs7OzJ8/P+PHj3/D7S655JJccMEFmTt3bsaMGfOW3/P000/nhRdeyODBg4usGwAAAAD+VFVDW5K0tLTk6quvzne+85088sgj+fznP581a9bkhBNOSJJMnTq128MSLr744pxzzjn59re/nWHDhqW1tTWtra1ZvXp1kmT16tX54he/mLvvvjtPPvlk5s+fn6OOOirDhw/PhAkTqnKMAAAAAGz9qn6PtsmTJ+e5557L9OnT09ramlGjRmXu3LldD0hYsmRJt6c5XHXVVVm3bl0++clPdtvPjBkzcu6556Znz5554IEH8p3vfCcrV65MY2NjDj300FxwwQXr/XkoAAAAAJRQU6lUKtVexOamvb09dXV1aWtrc482AAAAgPe4DW1FVf/pKAAAAABsDYQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChgswhtV1xxRYYNG5Y+ffqkqakpixYtesO5V199dQ444IAMGDAgAwYMSHNz8+vmVyqVTJ8+PYMHD84222yT5ubmPPbYY5v6MAAAAAB4D6t6aLvhhhvS0tKSGTNm5N57783IkSMzYcKErFixYr3zFyxYkClTpuSOO+7IwoULM2TIkBx66KF55plnuuZccsklufzyyzN79uzcc8892W677TJhwoS8/PLL79ZhAQAAAPAeU1OpVCrVXEBTU1PGjh2bWbNmJUk6OzszZMiQnHzyyTnzzDPfcvuOjo4MGDAgs2bNytSpU1OpVNLY2JjTTjstp59+epKkra0t9fX1mTNnTo499ti33Gd7e3vq6urS1taW/v37v7MDBAAAAGCLtqGtqKpXtK1bty6LFy9Oc3Nz11iPHj3S3NychQsXbtA+XnrppbzyyivZYYcdkiRPPPFEWltbu+2zrq4uTU1Nb7jPtWvXpr29vdsLAAAAAN6Oqoa2559/Ph0dHamvr+82Xl9fn9bW1g3ax5e+9KU0NjZ2hbXXtns7+5w5c2bq6uq6XkOGDHm7hwIAAADAe1zV79H2Tlx00UW5/vrrc9NNN6VPnz4bvZ9p06alra2t67V06dKCqwQAAADgvaBXNb980KBB6dmzZ5YvX95tfPny5WloaHjTbS+99NJcdNFFuf322zNixIiu8de2W758eQYPHtxtn6NGjVrvvmpra1NbW7uRRwEAAAAAVb6irXfv3hk9enTmz5/fNdbZ2Zn58+dn/Pjxb7jdJZdckgsuuCBz587NmDFjun222267paGhods+29vbc88997zpPgEAAADgnajqFW1J0tLSkuOOOy5jxozJuHHjctlll2XNmjU54YQTkiRTp07NzjvvnJkzZyZJLr744kyfPj3XXntthg0b1nXftb59+6Zv376pqanJqaeemgsvvDB77rlndtttt5xzzjlpbGzMpEmTqnWYAAAAAGzlqh7aJk+enOeeey7Tp09Pa2trRo0alblz53Y9zGDJkiXp0eOPF95dddVVWbduXT75yU9228+MGTNy7rnnJknOOOOMrFmzJp/97GezcuXK7L///pk7d+47uo8bAAAAALyZmkqlUqn2IjY37e3tqaurS1tbW/r371/t5QAAAABQRRvairbop44CAAAAwOZCaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAoQ2AAAAAChAaAMAAACAAqoe2q644ooMGzYsffr0SVNTUxYtWvSGcx966KEcc8wxGTZsWGpqanLZZZe9bs65556bmpqabq+99957Ex4BAAAAAFQ5tN1www1paWnJjBkzcu+992bkyJGZMGFCVqxYsd75L730UnbfffdcdNFFaWhoeMP97rPPPlm2bFnX6xe/+MWmOgQAAAAASFLl0Pb1r389f/M3f5MTTjghH/zgBzN79uxsu+22+fa3v73e+WPHjs1Xv/rVHHvssamtrX3D/fbq1SsNDQ1dr0GDBm2qQwAAAACAJFUMbevWrcvixYvT3Nz8x8X06JHm5uYsXLjwHe37scceS2NjY3bfffd86lOfypIlS950/tq1a9Pe3t7tBQAAAABvR9VC2/PPP5+Ojo7U19d3G6+vr09ra+tG77epqSlz5szJ3Llzc9VVV+WJJ57IAQcckFWrVr3hNjNnzkxdXV3Xa8iQIRv9/QAAAAC8N1X9YQilHXbYYfmrv/qrjBgxIhMmTMitt96alStX5oc//OEbbjNt2rS0tbV1vZYuXfourhgAAACArUGvan3xoEGD0rNnzyxfvrzb+PLly9/0QQdv1/bbb5/3v//9efzxx99wTm1t7Zve8w0AAAAA3krVrmjr3bt3Ro8enfnz53eNdXZ2Zv78+Rk/fnyx71m9enV+97vfZfDgwcX2CQAAAAB/qmpXtCVJS0tLjjvuuIwZMybjxo3LZZddljVr1uSEE05IkkydOjU777xzZs6cmeTVByg8/PDDXX9+5plncv/996dv374ZPnx4kuT000/PEUcckaFDh+bZZ5/NjBkz0rNnz0yZMqU6BwkAAADAe0JVQ9vkyZPz3HPPZfr06Wltbc2oUaMyd+7crgckLFmyJD16/PGiu2effTYf/vCHu95feumlufTSS3PggQdmwYIFSZKnn346U6ZMyQsvvJAdd9wx+++/f+6+++7suOOO7+qxAQAAAPDeUlOpVCrVXsTmpr29PXV1dWlra0v//v2rvRwAAAAAqmhDW9FW99RRAAAAAKgGoQ0AAAAAChDaAAAAAKAAoQ0AAAAAChDaAAAAAKAAoQ0AAAAAChDaAAAAAKAAoQ0AAAAAChDaAAAAAKAAoQ0AAAAAChDaAAAAAKAAoQ0AAAAAChDaAAAAAKAAoQ0AAAAAChDaAAAAAKAAoQ0AAAAAChDaAAAAAKAAoQ0AAAAAChDaAAAAAKAAoQ0AAAAAChDaAAAAAKAAoQ0AAAAAChDaAAAAAKAAoQ0AAAAAChDaAAAAAKAAoQ0AAAAAChDaAAAAAKAAoQ0AAAAAChDaAAAAAKAAoQ0AAAAAChDaAAAAAKAAoQ0AAAAAChDaAAAAAKAAoQ0AAAAAChDaAAAAAKAAoQ0AAAAAChDaAAAAAKAAoQ0AAAAAChDaAAAAAKAAoQ0AAAAAChDaAAAAAKAAoQ0AAAAAChDaAAAAAKCAjQptS5cuzdNPP931ftGiRTn11FPzrW99q9jCAAAAAGBLslGh7a//+q9zxx13JElaW1vz8Y9/PIsWLcpZZ52V888/v+gCAQAAAGBLsFGh7cEHH8y4ceOSJD/84Q+z77775q677soPfvCDzJkzp+T6AAAAAGCLsFGh7ZVXXkltbW2S5Pbbb8+RRx6ZJNl7772zbNmycqsDAAAAgC3ERoW2ffbZJ7Nnz87/+3//L/PmzcvEiROTJM8++2wGDhxYdIEAAAAAsCXYqNB28cUX55vf/GYOOuigTJkyJSNHjkyS/PjHP+76SSkAAAAAvJfUVCqVysZs2NHRkfb29gwYMKBr7Mknn8y2226bnXbaqdgCq6G9vT11dXVpa2tL//79q70cAAAAAKpoQ1vRRl3R9vvf/z5r167timxPPfVULrvssvz2t7/d4iMbAAAAAGyMjQptRx11VL773e8mSVauXJmmpqZ87Wtfy6RJk3LVVVcVXSAAAAAAbAk2KrTde++9OeCAA5Ik//zP/5z6+vo89dRT+e53v5vLL7+86AIBAAAAYEuwUaHtpZdeSr9+/ZIkP/3pT3P00UenR48e+ehHP5qnnnqq6AIBAAAAYEuwUaFt+PDhufnmm7N06dLcdtttOfTQQ5MkK1as8PAAAAAAAN6TNiq0TZ8+PaeffnqGDRuWcePGZfz48Ulevbrtwx/+cNEFAgAAAMCWoKZSqVQ2ZsPW1tYsW7YsI0eOTI8er/a6RYsWpX///tl7772LLvLdtqGPbAUAAABg67ehrajXxn5BQ0NDGhoa8vTTTydJdtlll4wbN25jdwcAAAAAW7SN+uloZ2dnzj///NTV1WXo0KEZOnRott9++1xwwQXp7OwsvUYAAAAA2Oxt1BVtZ511Vv7P//k/ueiii7LffvslSX7xi1/k3HPPzcsvv5yvfOUrRRcJAAAAAJu7jbpHW2NjY2bPnp0jjzyy2/iPfvSj/N3f/V2eeeaZYgusBvdoAwAAAOA1G9qKNuqnoy+++OJ6H3iw995758UXX9yYXQIAAADAFm2jQtvIkSMza9as143PmjUrI0aMeMeLAgAAAIAtzUbdo+2SSy7J4Ycfnttvvz3jx49PkixcuDBLly7NrbfeWnSBAAAAALAl2Kgr2g488MD8x3/8R/7yL/8yK1euzMqVK3P00UfnoYceyve+973SawQAAACAzd5GPQzhjfz617/ORz7ykXR0dJTaZVV4GAIAAAAAr9mkD0MAAAAAALoT2gAAAACgAKENAAAAAAp4W08dPfroo9/085UrV76TtQAAAADAFutthba6urq3/Hzq1KnvaEEAAAAAsCV6W6Htmmuu2VTrAAAAAIAtmnu0AQAAAEABQhsAAAAAFCC0AQAAAEABQhsAAAAAFCC0AQAAAEABQhsAAAAAFCC0AQAAAEABQhsAAAAAFCC0AQAAAEABQhsAAAAAFCC0AQAAAEABQhsAAAAAFCC0AQAAAEABQhsAAAAAFCC0AQAAAEABQhsAAAAAFCC0AQAAAEABQhsAAAAAFCC0AQAAAEABQhsAAAAAFCC0AQAAAEABQhsAAAAAFCC0AQAAAEABQhsAAAAAFCC0AQAAAEABQhsAAAAAFCC0AQAAAEABQhsAAAAAFCC0AQAAAEABQhsAAAAAFCC0AQAAAEABQhsAAAAAFCC0AQAAAEABQhsAAAAAFCC0AQAAAEABQhsAAAAAFCC0AQAAAEABQhsAAAAAFCC0AQAAAEABQhsAAAAAFFD10HbFFVdk2LBh6dOnT5qamrJo0aI3nPvQQw/lmGOOybBhw1JTU5PLLrvsHe8TAAAAAEqoami74YYb0tLSkhkzZuTee+/NyJEjM2HChKxYsWK981966aXsvvvuueiii9LQ0FBknwAAAABQQk2lUqlU68ubmpoyduzYzJo1K0nS2dmZIUOG5OSTT86ZZ575ptsOGzYsp556ak499dRi+3xNe3t76urq0tbWlv79+7/9AwMAAABgq7GhrahqV7StW7cuixcvTnNz8x8X06NHmpubs3Dhwnd1n2vXrk17e3u3FwAAAAC8HVULbc8//3w6OjpSX1/fbby+vj6tra3v6j5nzpyZurq6rteQIUM26vsBAAAAeO+q+sMQNgfTpk1LW1tb12vp0qXVXhIAAAAAW5he1friQYMGpWfPnlm+fHm38eXLl7/hgw421T5ra2tTW1u7Ud8JAAAAAEkVr2jr3bt3Ro8enfnz53eNdXZ2Zv78+Rk/fvxms08AAAAA2BBVu6ItSVpaWnLcccdlzJgxGTduXC677LKsWbMmJ5xwQpJk6tSp2XnnnTNz5swkrz7s4OGHH+768zPPPJP7778/ffv2zfDhwzdonwAAAACwKVQ1tE2ePDnPPfdcpk+fntbW1owaNSpz587tepjBkiVL0qPHHy+6e/bZZ/PhD3+46/2ll16aSy+9NAceeGAWLFiwQfsEAAAAgE2hplKpVKq9iM1Ne3t76urq0tbWlv79+1d7OQAAAABU0Ya2Ik8dBQAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIACNovQdsUVV2TYsGHp06dPmpqasmjRojedf+ONN2bvvfdOnz598qEPfSi33nprt8+PP/741NTUdHtNnDhxUx4CAAAAAO9xVQ9tN9xwQ1paWjJjxozce++9GTlyZCZMmJAVK1asd/5dd92VKVOm5DOf+Uzuu+++TJo0KZMmTcqDDz7Ybd7EiROzbNmyrtd11133bhwOAAAAAO9RNZVKpVLNBTQ1NWXs2LGZNWtWkqSzszNDhgzJySefnDPPPPN18ydPnpw1a9bklltu6Rr76Ec/mlGjRmX27NlJXr2ibeXKlbn55ps3ak3t7e2pq6tLW1tb+vfvv1H7AAAAAGDrsKGtqKpXtK1bty6LFy9Oc3Nz11iPHj3S3NychQsXrnebhQsXdpufJBMmTHjd/AULFmSnnXbKXnvtlc9//vN54YUX3nAda9euTXt7e7cXAAAAALwdVQ1tzz//fDo6OlJfX99tvL6+Pq2trevdprW19S3nT5w4Md/97nczf/78XHzxxbnzzjtz2GGHpaOjY737nDlzZurq6rpeQ4YMeYdHBgAAAMB7Ta9qL2BTOPbYY7v+/KEPfSgjRozIHnvskQULFuSQQw553fxp06alpaWl6317e7vYBgAAAMDbUtUr2gYNGpSePXtm+fLl3caXL1+ehoaG9W7T0NDwtuYnye67755Bgwbl8ccfX+/ntbW16d+/f7cXAAAAALwdVQ1tvXv3zujRozN//vyusc7OzsyfPz/jx49f7zbjx4/vNj9J5s2b94bzk+Tpp5/OCy+8kMGDB5dZOAAAAAD8iaqGtiRpaWnJ1Vdfne985zt55JFH8vnPfz5r1qzJCSeckCSZOnVqpk2b1jX/lFNOydy5c/O1r30tjz76aM4999z86le/ykknnZQkWb16db74xS/m7rvvzpNPPpn58+fnqKOOyvDhwzNhwoSqHCMAAAAAW7+q36Nt8uTJee655zJ9+vS0trZm1KhRmTt3btcDD5YsWZIePf7YAz/2sY/l2muvzdlnn50vf/nL2XPPPXPzzTdn3333TZL07NkzDzzwQL7zne9k5cqVaWxszKGHHpoLLrggtbW1VTlGAAAAALZ+NZVKpVLtRWxu2tvbU1dXl7a2NvdrAwAAAHiP29BWVPWfjgIAAADA1kBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIAChDYAAAAAKEBoAwAAAIACNovQdsUVV2TYsGHp06dPmpqasmjRojedf+ONN2bvvfdOnz598qEPfSi33nprt88rlUqmT5+ewYMHZ5tttklzc3Mee+yxTXkIAAAAALzHVT203XDDDWlpacmMGTNy7733ZuTIkZkwYUJWrFix3vl33XVXpkyZks985jO57777MmnSpEyaNCkPPvhg15xLLrkkl19+eWbPnp177rkn2223XSZMmJCXX3753TosAAAAAN5jaiqVSqWaC2hqasrYsWMza9asJElnZ2eGDBmSk08+OWeeeebr5k+ePDlr1qzJLbfc0jX20Y9+NKNGjcrs2bNTqVTS2NiY0047LaeffnqSpK2tLfX19ZkzZ06OPfbYt1xTe3t76urq0tbWlv79+xc6UgAAAAC2RBvainq9i2t6nXXr1mXx4sWZNm1a11iPHj3S3NychQsXrnebhQsXpqWlpdvYhAkTcvPNNydJnnjiibS2tqa5ubnr87q6ujQ1NWXhwoXrDW1r167N2rVru963tbUlefUvEQAAAID3ttca0Vtdr1bV0Pb888+no6Mj9fX13cbr6+vz6KOPrneb1tbW9c5vbW3t+vy1sTea86dmzpyZ884773XjQ4YM2bADAQAAAGCrt2rVqtTV1b3h51UNbZuLadOmdbtKrrOzMy+++GIGDhyYmpqaKq6snPb29gwZMiRLly71c1gozPkFm4ZzCzYd5xdsOs4v2DSqfW5VKpWsWrUqjY2NbzqvqqFt0KBB6dmzZ5YvX95tfPny5WloaFjvNg0NDW86/7X/XL58eQYPHtxtzqhRo9a7z9ra2tTW1nYb23777d/OoWwx+vfv77/sYRNxfsGm4dyCTcf5BZuO8ws2jWqeW292JdtrqvrU0d69e2f06NGZP39+11hnZ2fmz5+f8ePHr3eb8ePHd5ufJPPmzeuav9tuu6WhoaHbnPb29txzzz1vuE8AAAAAeKeq/tPRlpaWHHfccRkzZkzGjRuXyy67LGvWrMkJJ5yQJJk6dWp23nnnzJw5M0lyyimn5MADD8zXvva1HH744bn++uvzq1/9Kt/61reSJDU1NTn11FNz4YUXZs8998xuu+2Wc845J42NjZk0aVK1DhMAAACArVzVQ9vkyZPz3HPPZfr06Wltbc2oUaMyd+7crocZLFmyJD16/PHCu4997GO59tprc/bZZ+fLX/5y9txzz9x8883Zd999u+acccYZWbNmTT772c9m5cqV2X///TN37tz06dPnXT++zUVtbW1mzJjxup/IAu+c8ws2DecWbDrOL9h0nF+waWwp51ZN5a2eSwoAAAAAvKWq3qMNAAAAALYWQhsAAAAAFCC0AQAAAEABQhsAAAAAFCC0vQdcccUVGTZsWPr06ZOmpqYsWrSo2kuCLc7Pf/7zHHHEEWlsbExNTU1uvvnmbp9XKpVMnz49gwcPzjbbbJPm5uY89thj1VksbEFmzpyZsWPHpl+/ftlpp50yadKk/Pa3v+025+WXX86JJ56YgQMHpm/fvjnmmGOyfPnyKq0YthxXXXVVRowYkf79+6d///4ZP358fvKTn3R97tyCci666KLU1NTk1FNP7RpzjsHGOffcc1NTU9Pttffee3d9vrmfW0LbVu6GG25IS0tLZsyYkXvvvTcjR47MhAkTsmLFimovDbYoa9asyciRI3PFFVes9/NLLrkkl19+eWbPnp177rkn2223XSZMmJCXX375XV4pbFnuvPPOnHjiibn77rszb968vPLKKzn00EOzZs2arjl///d/n3/913/NjTfemDvvvDPPPvtsjj766CquGrYMu+yySy666KIsXrw4v/rVr/Lnf/7nOeqoo/LQQw8lcW5BKb/85S/zzW9+MyNGjOg27hyDjbfPPvtk2bJlXa9f/OIXXZ9t9udWha3auHHjKieeeGLX+46OjkpjY2Nl5syZVVwVbNmSVG666aau952dnZWGhobKV7/61a6xlStXVmprayvXXXddFVYIW64VK1ZUklTuvPPOSqXy6rn0vve9r3LjjTd2zXnkkUcqSSoLFy6s1jJhizVgwIDKP/3TPzm3oJBVq1ZV9txzz8q8efMqBx54YOWUU06pVCr+/YJ3YsaMGZWRI0eu97Mt4dxyRdtWbN26dVm8eHGam5u7xnr06JHm5uYsXLiwiiuDrcsTTzyR1tbWbudaXV1dmpqanGvwNrW1tSVJdthhhyTJ4sWL88orr3Q7v/bee+/suuuuzi94Gzo6OnL99ddnzZo1GT9+vHMLCjnxxBNz+OGHdzuXEv9+wTv12GOPpbGxMbvvvns+9alPZcmSJUm2jHOrV7UXwKbz/PPPp6OjI/X19d3G6+vr8+ijj1ZpVbD1aW1tTZL1nmuvfQa8tc7Ozpx66qnZb7/9su+++yZ59fzq3bt3tt9++25znV+wYX7zm99k/Pjxefnll9O3b9/cdNNN+eAHP5j777/fuQXv0PXXX5977703v/zlL1/3mX+/YOM1NTVlzpw52WuvvbJs2bKcd955OeCAA/Lggw9uEeeW0AYAbBZOPPHEPPjgg93uwQG8M3vttVfuv//+tLW15Z//+Z9z3HHH5c4776z2smCLt3Tp0pxyyimZN29e+vTpU+3lwFblsMMO6/rziBEj0tTUlKFDh+aHP/xhttlmmyqubMP46ehWbNCgQenZs+frnr6xfPnyNDQ0VGlVsPV57XxyrsHGO+mkk3LLLbfkjjvuyC677NI13tDQkHXr1mXlypXd5ju/YMP07t07w4cPz+jRozNz5syMHDky//iP/+jcgndo8eLFWbFiRT7ykY+kV69e6dWrV+68885cfvnl6dWrV+rr651jUMj222+f97///Xn88ce3iH+/hLatWO/evTN69OjMnz+/a6yzszPz58/P+PHjq7gy2LrstttuaWho6Hautbe355577nGuwVuoVCo56aSTctNNN+VnP/tZdtttt26fjx49Ou973/u6nV+//e1vs2TJEucXbITOzs6sXbvWuQXv0CGHHJLf/OY3uf/++7teY8aMyac+9amuPzvHoIzVq1fnd7/7XQYPHrxF/Pvlp6NbuZaWlhx33HEZM2ZMxo0bl8suuyxr1qzJCSecUO2lwRZl9erVefzxx7veP/HEE7n//vuzww47ZNddd82pp56aCy+8MHvuuWd22223nHPOOWlsbMykSZOqt2jYApx44om59tpr86Mf/Sj9+vXrurdGXV1dttlmm9TV1eUzn/lMWlpassMOO6R///45+eSTM378+Hz0ox+t8uph8zZt2rQcdthh2XXXXbNq1apce+21WbBgQW677TbnFrxD/fr167qf6Gu22267DBw4sGvcOQYb5/TTT88RRxyRoUOH5tlnn82MGTPSs2fPTJkyZYv490to28pNnjw5zz33XKZPn57W1taMGjUqc+fOfd1N24E396tf/SoHH3xw1/uWlpYkyXHHHZc5c+bkjDPOyJo1a/LZz342K1euzP7775+5c+e6Zwe8hauuuipJctBBB3Ubv+aaa3L88ccnSb7xjW+kR48eOeaYY7J27dpMmDAhV1555bu8UtjyrFixIlOnTs2yZctSV1eXESNG5LbbbsvHP/7xJM4t2NScY7Bxnn766UyZMiUvvPBCdtxxx+y///65++67s+OOOybZ/M+tmkqlUqn2IgAAAABgS+cebQAAAABQgNAGAAAAAAUIbQAAAABQgNAGAAAAAAUIbQAAAABQgNAGAAAAAAUIbQAAAABQgNAGAAAAAAUIbQAAFDNnzpxsv/321V4GAEBVCG0AAFuh448/PjU1NV2vgQMHZuLEiXnggQc2eB/nnntuRo0atekWCQCwlRHaAAC2UhMnTsyyZcuybNmyzJ8/P7169conPvGJai8LAGCrJbQBAGylamtr09DQkIaGhowaNSpnnnlmli5dmueeey5J8qUvfSnvf//7s+2222b33XfPOeeck1deeSXJqz8BPe+88/LrX/+666q4OXPmJElWrlyZz33uc6mvr0+fPn2y77775pZbbun23bfddls+8IEPpG/fvl3BDwBga9er2gsAAGDTW716db7//e9n+PDhGThwYJKkX79+mTNnThobG/Ob3/wmf/M3f5N+/frljDPOyOTJk/Pggw9m7ty5uf3225MkdXV16ezszGGHHZZVq1bl+9//fvbYY488/PDD6dmzZ9d3vfTSS7n00kvzve99Lz169MinP/3pnH766fnBD35QlWMHAHi3CG0AAFupW265JX379k2SrFmzJoMHD84tt9ySHj1e/VHD2Wef3TV32LBhOf3003P99dfnjDPOyDbbbJO+ffumV69eaWho6Jr305/+NIsWLcojjzyS97///UmS3Xffvdv3vvLKK5k9e3b22GOPJMlJJ52U888/f5MeKwDA5kBoAwDYSh188MG56qqrkiT/9V//lSuvvDKHHXZYFi1alKFDh+aGG27I5Zdfnt/97ndZvXp1/vCHP6R///5vus/7778/u+yyS1dkW59tt922K7IlyeDBg7NixYoyBwUAsBlzjzYAgK3Udtttl+HDh2f48OEZO3Zs/umf/ilr1qzJ1VdfnYULF+ZTn/pU/uIv/iK33HJL7rvvvpx11llZt27dm+5zm222ecvvfd/73tftfU1NTSqVyjs6FgCALYEr2gAA3iNqamrSo0eP/P73v89dd92VoUOH5qyzzur6/Kmnnuo2v3fv3uno6Og2NmLEiDz99NP5j//4jze9qg0A4L1IaAMA2EqtXbs2ra2tSV796eisWbOyevXqHHHEEWlvb8+SJUty/fXXZ+zYsfm3f/u33HTTTd22HzZsWJ544omun4v269cvBx54YP7sz/4sxxxzTL7+9a9n+PDhefTRR1NTU5OJEydW4zABADYbfjoKALCVmjt3bgYPHpzBgwenqakpv/zlL3PjjTfmoIMOypFHHpm///u/z0knnZRRo0blrrvuyjnnnNNt+2OOOSYTJ07MwQcfnB133DHXXXddkuRf/uVfMnbs2EyZMiUf/OAHc8YZZ7zuyjcAgPeimoobZgAAAADAO+aKNgAAAAAoQGgDAAAAgAKENgAAAAAoQGgDAAAAgAKENgAAAAAoQGgDAAAAgAKENgAAAAAoQGgDAAAAgAKENgAAAAAoQGgDAAAAgAKENgAAAAAo4P8Dajc/a+AlNa4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0,0.25)\n",
    "plt.plot([loss.item() for loss in train_loss_set])\n",
    "# plt.plot(train_loss_set)\n",
    "plt.savefig(\"pngs/5_05_BERT_CONLL_NER_1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wqic4K83FLKX",
    "outputId": "6b419548-c141-4cb0-fe46-0eb912c719ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.51308103118624\n",
      "Validation Accuracy: 0.9689880952380953\n",
      "Validation F1-Score: 0.21757322175732216\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "for batch in valid_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    b_labels = b_labels.long()\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                              attention_mask=b_input_mask, labels=b_labels)\n",
    "        logits = model(b_input_ids, token_type_ids=None,\n",
    "                       attention_mask=b_input_mask)\n",
    "        \n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    true_labels.append(label_ids)\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += b_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "pred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]\n",
    "valid_tags = [[tags_vals[l_ii] for l_ii in l_i] for l in true_labels for l_i in l ]\n",
    "print(\"Validation loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "05_BERT_CONLL_NER.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
