{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nDRrAaF6B0Qd"
   },
   "source": [
    "## Text Summarization\n",
    "\n",
    "### There are broadly two types of summarization — Extractive and Abstractive\n",
    "\n",
    "    1. Extractive— These approaches select sentences from the corpus that best represent it and arrange them to form a summary.\n",
    "    2. Abstractive— These approaches use natural language techniques to summarize a text using novel sentences.\n",
    "\n",
    "In this notebook, let us see a few examples of existing summarization approaches.\n",
    "The first one comes from the python library sumy, which implements several popular summarization approaches from literature. The second example uses gensim's summarizer implementation. Then we move on to Summa and finally we wrap up extractive summarization using BERT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XlIo08bpB0Qm"
   },
   "source": [
    "## Summarization with Sumy\n",
    "\n",
    "### Sumy offers several algorithms and methods for summarization such as:\n",
    "\n",
    "\n",
    "\n",
    "    1. Luhn – Heurestic method\n",
    "    2. Latent Semantic Analysis\n",
    "    4. LexRank – Unsupervised approach inspired by algorithms PageRank and HITS\n",
    "    5. TextRank - Graph-based summarization technique with keyword extractions in from document\n",
    "There are many more which you can find in the github repo of [sumy](https://github.com/miso-belica/sumy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fE-e9HKKB0Qv",
    "outputId": "fcecd5af-f0d5-4245-b3f3-849815c26fb3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install sumy\n",
    "\n",
    "# !pip install sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZVKqAHCEB0R0",
    "outputId": "8723b0bf-d3a8-4d08-de38-61c0d242647b"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# For NLTK virtual environments are high recommended and it requires python verisions higher than 3.5 on windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SVu_YDlXB0SR",
    "outputId": "396bccc7-eeeb-43c4-dc60-a4c718d2dd67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextRankSummarizer:\n",
      "For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.\n",
      "A Class of Submodular Functions for Document Summarization\", The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), 2011^ Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, Learning Mixtures of Submodular Functions for Image Collection Summarization, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.^ Ramakrishna Bairi, Rishabh Iyer, Ganesh Ramakrishnan and Jeff Bilmes, Summarizing Multi-Document Topic Hierarchies using Submodular Mixtures, To Appear In the Annual Meeting of the Association for Computational Linguistics (ACL), Beijing, China, July - 2015^ Kai Wei, Rishabh Iyer, and Jeff Bilmes, Submodularity in Data Subset Selection and Active Learning Archived 2017-03-13 at the Wayback Machine, To Appear In Proc.\n",
      "------------------------------\n",
      "LexRankSummarizer:\n",
      "The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\".\n",
      "Automatic Text Summarization.\n",
      "------------------------------\n",
      "LuhnSummarizer:\n",
      "Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).\n",
      "A Class of Submodular Functions for Document Summarization\", The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), 2011^ Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, Learning Mixtures of Submodular Functions for Image Collection Summarization, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.^ Ramakrishna Bairi, Rishabh Iyer, Ganesh Ramakrishnan and Jeff Bilmes, Summarizing Multi-Document Topic Hierarchies using Submodular Mixtures, To Appear In the Annual Meeting of the Association for Computational Linguistics (ACL), Beijing, China, July - 2015^ Kai Wei, Rishabh Iyer, and Jeff Bilmes, Submodularity in Data Subset Selection and Active Learning Archived 2017-03-13 at the Wayback Machine, To Appear In Proc.\n",
      "------------------------------\n",
      "LsaSummarizer\n",
      "Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney's seminal paper.\n",
      "Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity.\n",
      "------------------------------\n",
      "original\n",
      "The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\".\n",
      "Automatic Text Summarization.\n",
      "------------------------------\n",
      "with no greedines\n",
      "https://www.dummies.com/education/language-arts/speed-reading/how-to-skim-text/ Accessed Dec 2019.^ a b Afzal M, Alam F, Malik KM, Malik GM, Clinical Context-Aware Biomedical Text Summarization Using Deep Neural Network: Model Development and Validation, J Med Internet Res 2020;22(10):e19810, DOI: 10.2196/19810, PMID 33095174^ Zhai, ChengXiang (2016).\n",
      "A Class of Submodular Functions for Document Summarization\", The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), 2011^ Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, Learning Mixtures of Submodular Functions for Image Collection Summarization, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.^ Ramakrishna Bairi, Rishabh Iyer, Ganesh Ramakrishnan and Jeff Bilmes, Summarizing Multi-Document Topic Hierarchies using Submodular Mixtures, To Appear In the Annual Meeting of the Association for Computational Linguistics (ACL), Beijing, China, July - 2015^ Kai Wei, Rishabh Iyer, and Jeff Bilmes, Submodularity in Data Subset Selection and Active Learning Archived 2017-03-13 at the Wayback Machine, To Appear In Proc.\n",
      "------------------------------\n",
      "with a lot of greedines\n",
      "Computer-based method for summarizing a text\n",
      "Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Code to summarize a given webpage using Sumy's TextRank implementation. \n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "num_sentences_in_summary = 2\n",
    "url = \"https://en.wikipedia.org/wiki/Automatic_summarization\"\n",
    "parser = HtmlParser.from_url(url, Tokenizer(\"english\"))\n",
    "\n",
    "summarizer_list=(\"TextRankSummarizer:\",\"LexRankSummarizer:\",\"LuhnSummarizer:\",\"LsaSummarizer\") #list of summarizers\n",
    "summarizers = [TextRankSummarizer(), LexRankSummarizer(), LuhnSummarizer(), LsaSummarizer()]\n",
    "\n",
    "for i,summarizer in enumerate(summarizers):\n",
    "    print(summarizer_list[i])\n",
    "    for sentence in summarizer(parser.document, num_sentences_in_summary):\n",
    "        print((sentence))\n",
    "    print(\"-\"*30)\n",
    "\n",
    "\n",
    "summarizer = LexRankSummarizer() \n",
    "print(\"original\")\n",
    "for sentence in summarizer(parser.document, num_sentences_in_summary):\n",
    "    print((sentence))\n",
    "print(\"-\"*30)\n",
    "\n",
    "print(\"with no greedines\")\n",
    "summarizer.threshold = 0.00001\n",
    "for sentence in summarizer(parser.document, num_sentences_in_summary):\n",
    "    print((sentence))\n",
    "print(\"-\"*30)\n",
    "\n",
    "print(\"with a lot of greedines\")\n",
    "summarizer.threshold = 0.99\n",
    "for sentence in summarizer(parser.document, num_sentences_in_summary):\n",
    "    print((sentence))\n",
    "print(\"-\"*30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i5Qyd9M2B0Ss"
   },
   "source": [
    "Clearly there are other summarizers and options in sumy. We leave their exploration as an exercise to you!\n",
    "\n",
    "## Summarization example with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TZOIdGesB0Sy",
    "outputId": "373be4d4-565b-46a2-be01-d3ccbb8e3fc5"
   },
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YB6IqAnrB0TL"
   },
   "source": [
    "Gensim does not have a HTML parser like sumy. So, let us use the example text from Chapter 5 (nlphistory.txt) to see what its summarized version looks like! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8dHkUgCoB0TP",
    "outputId": "5f2e675e-7be7-47a7-8bd4-3a7077aef9dc"
   },
   "outputs": [],
   "source": [
    "# NOTE: gensim summarization deprecated\n",
    "# from gensim.summarization import summarize,summarize_corpus\n",
    "# from gensim.summarization.textcleaner import split_sentences\n",
    "# from gensim import corpora\n",
    "\n",
    "# text = open(\"../data/nlphistory.txt\").read()\n",
    "\n",
    "# #summarize method extracts the most relevant sentences in a text\n",
    "# print(\"Summarize:\\n\",summarize(text, word_count=200, ratio = 0.1))\n",
    "\n",
    "\n",
    "# #the summarize_corpus selects the most important documents in a corpus:\n",
    "# sentences = split_sentences(text)# Creates a corpus where each document is a sentence.\n",
    "# tokens = [sentence.split() for sentence in sentences]\n",
    "# dictionary = corpora.Dictionary(tokens)\n",
    "# corpus = [dictionary.doc2bow(sentence_tokens) for sentence_tokens in tokens]\n",
    "\n",
    "# Extracts the most important documents (shown here in BoW representation)\n",
    "# print(\"-\"*30,\"\\nSummarize Corpus\\n\",summarize_corpus(corpus,ratio=0.1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5XGqBqPQB0T7"
   },
   "source": [
    "The two parameters **word_count** and **ratio** we can adjust how much text the summarizer outputs\n",
    "1. word_count: maximum amount of words we want in the summary\n",
    "2. ratio: fraction of sentences in the original text should be returned as output\n",
    "\n",
    "### Todo: Explore other options in gensim summarizer, what are possible shortcomings (e.g., sensitive to input's format etc)\n",
    "[Short-Comings\n",
    "1. gensim's summarizer uses TextRank by default, an algorithm that uses PageRank. In gensim it is unfortunately implemented using a Python list of PageRank graph nodes, so it may fail if your graph is too big.]\n",
    "\n",
    "\n",
    "\n",
    "## Summa Summarizer\n",
    "The summa summarizer uses TextRank too but with optimizations on similar functions. More information about the optimizations can be found in the following [paper](https://arxiv.org/pdf/1602.03606.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5p5qBTf7B0T_",
    "outputId": "76dad3b1-099d-4306-e5e3-0eae8b1ffcf3"
   },
   "outputs": [],
   "source": [
    "# !pip install summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EcBq0k1mB0UX",
    "outputId": "4c8f3982-cbd9-4850-f298-9318746af1f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data.\n",
      "In the 2010s, representation learning and deep neural network-style machine learning methods became widespread in natural language processing, due in part to a flurry of results showing that such techniques[4][5] can achieve state-of-the-art results in many natural language tasks, for example in language modeling,[6] parsing,[7][8] and many others.\n"
     ]
    }
   ],
   "source": [
    "from summa import summarizer\n",
    "from summa import keywords\n",
    "text = open(\"../data/nlphistory.txt\").read()\n",
    "\n",
    "print(\"Summary:\")\n",
    "print (summarizer.summarize(text,ratio=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0_x7UTe3B0Up"
   },
   "source": [
    "### BERT for Extractive Summarization\n",
    "Lets see how we can use BERT for extractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iEGOmpwjB0VE",
    "outputId": "a13ba70e-859f-467a-d2c7-48ab3db91c56"
   },
   "outputs": [],
   "source": [
    "#Install the required libraries\n",
    "# !pip install bert-extractive-summarizer=0.4.2\n",
    "# !pip install spacy==2.1.3\n",
    "# !pip install transformers==2.2.2\n",
    "# !pip install neuralcoref\n",
    "# !pip install torch #you can comment this line if u already have tensorflow2.0 installed\n",
    "# !pip install neuralcoref --no-binary neuralcoref\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aAmOJjZvB0VX",
    "outputId": "e875e731-e434-49e6-f49c-87d0ca0c699a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mccar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mccar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "loading configuration file config.json from cache at C:\\Users\\mccar/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\mccar/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\model.safetensors\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at C:\\Users\\mccar/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\mccar/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\mccar/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "C:\\Users\\mccar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "His parents were very hard-workingand deeply religious people, but so poor that they lived with their fivechildren in only two rooms. In June 1880 he made his famous speech at the unveiling of themonument to Pushkin in Moscow and he was received with extraordinarydemonstrations of love and honour. In the words of a Russian critic, who seeks to explain the feelinginspired by Dostoevsky: “He was one of ourselves, a man of our blood andour bone, but one who has suffered and has seen so much more deeply thanwe have his insight impresses us as wisdom... that wisdom of the heartwhich we seek that we may learn from it how to live. ”CRIME AND PUNISHMENTPART ICHAPTER IOn an exceptionally hot evening early in July a young man came out ofthe garret in which he lodged in S. Place and walked slowly, as thoughin hesitation, towards K. bridge.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\mccar/.cache\\huggingface\\hub\\models--bert-large-uncased\\snapshots\\80792f8e8216b29f3c846b653a0ff0a37c210431\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\mccar/.cache\\huggingface\\hub\\models--bert-large-uncased\\snapshots\\80792f8e8216b29f3c846b653a0ff0a37c210431\\model.safetensors\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at C:\\Users\\mccar/.cache\\huggingface\\hub\\models--bert-large-uncased\\snapshots\\80792f8e8216b29f3c846b653a0ff0a37c210431\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\mccar/.cache\\huggingface\\hub\\models--bert-large-uncased\\snapshots\\80792f8e8216b29f3c846b653a0ff0a37c210431\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\mccar/.cache\\huggingface\\hub\\models--bert-large-uncased\\snapshots\\80792f8e8216b29f3c846b653a0ff0a37c210431\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "C:\\Users\\mccar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "His parents were very hard-workingand deeply religious people, but so poor that they lived with their fivechildren in only two rooms. Thereupon we were bound in threes to stakes,to suffer execution. Though his religious temper led him in the end toaccept every suffering with resignation and to regard it as a blessingin his own case, he constantly recurs to the subject in his writings. In 1859 he wasallowed to return to Russia. He startedanother journal--“The Epoch,” which within a few months was alsoprohibited. He was weighed down by debt, his brother’s family wasdependent on him, he was forced to write at heart-breaking speed, and issaid never to have corrected his work. He is still probably the most widely read writer in Russia. In the words of a Russian critic, who seeks to explain the feelinginspired by Dostoevsky: “He was one of ourselves, a man of our blood andour bone, but one who has suffered and has seen so much more deeply thanwe have his insight impresses us as wisdom... that wisdom of the heartwhich we seek that we may learn from it how to live. Hisgarret was under the roof of a high, five-storied house and was morelike a cupboard than a room.\n",
      "His parents were very hard-workingand deeply religious people, but so poor that they lived with their fivechildren in only two rooms. The father and mother spent their eveningsin reading aloud to their children, generally from books of a seriouscharacter. Though his religious temper led him in the end toaccept every suffering with resignation and to regard it as a blessingin his own case, he constantly recurs to the subject in his writings. In the words of a Russian critic, who seeks to explain the feelinginspired by Dostoevsky: “He was one of ourselves, a man of our blood andour bone, but one who has suffered and has seen so much more deeply thanwe have his insight impresses us as wisdom... that wisdom of the heartwhich we seek that we may learn from it how to live.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mccar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "#sowyma could you please look at this coreference vs without coreference. I personally think we need to use a better input.\n",
    "#currently using the same one as above the nlphistory.txt\n",
    "\n",
    "# summarizer is the bert-extractive-summarizer\n",
    "# from summarizer import Summarizer\n",
    "# from summarizer.coreference_handler import CoreferenceHandler\n",
    "\n",
    "# model = Summarizer()\n",
    "\n",
    "# print(\"Without Coreference:\")\n",
    "# result = model(text, min_length=200,ratio=0.01)\n",
    "# full = ''.join(result)\n",
    "# print(full)\n",
    "\n",
    "\n",
    "# print(\"With Coreference:\")\n",
    "# handler = CoreferenceHandler(greedyness=.35)\n",
    "# model = Summarizer(sentence_handler=handler)\n",
    "# result = model(text, min_length=200,ratio=0.01)\n",
    "# full = ''.join(result)\n",
    "# print(full)\n",
    "\n",
    "with open(\"../data/text.txt\", 'r', encoding=\"UTF-8\") as file:\n",
    "    body = file.read().replace('\\n', '')\n",
    "\n",
    "\n",
    "# THIS WORKS (8/16/2023):\n",
    "from summarizer import Summarizer\n",
    "# body = 'This text is about donkeys. Its also about chickens.'\n",
    "model = Summarizer('distilbert-base-uncased', hidden=[-1,-2], hidden_concat=True)\n",
    "result = model(body, num_sentences=3)\n",
    "print(result)\n",
    "\n",
    "# body = 'Text body that you want to summarize with BERT'\n",
    "model = Summarizer()\n",
    "result = model(body, ratio=0.2)  # Specified with ratio\n",
    "print(result)\n",
    "result = model(body, num_sentences=3)  # Will return 3 sentences \n",
    "print(result)\n",
    "\n",
    "# print(\"With Coreference:\")\n",
    "# NOTE: this does not work. Neuralcoref not compatible with the new spacy, see above for a greediness / threshold example\n",
    "# handler = CoreferenceHandler(greedyness=.35)\n",
    "\n",
    "# model = Summarizer(sentence_handler=handler)\n",
    "# result = model(text, min_length=200,ratio=0.01)\n",
    "# full = ''.join(result)\n",
    "# print(full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5OwLSKz0B0Vx"
   },
   "source": [
    "We are done with discussing different Extractive Summarization techniques and examples. Lets move on to Abstractive Summarization.\n",
    "## Abstractive Summariazation\n",
    "There have been even efforts to use **RL** for summarization.<br>\n",
    "The past few years **RNN**s using encoder — decoder models have become popular for abstractive summarization. <br>\n",
    "Recently **Transformers** which use attention mechanism have become popular for abstractive summarization. \n",
    "\n",
    "As mentioned in Ch7  abstractive summarization is more of a research topic than a practical application. \n",
    "\n",
    "We will demo simple abstractive text summarization with pretrained T5 — Text-To-Text Transfer Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "94W3znKkB0V0",
    "outputId": "a0ddec6f-8282-416f-c462-381db55e6083"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "Z7sj1wDtGz5r",
    "outputId": "2f24608f-4492-4d97-cd92-88c1aa3f19b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\mccar/.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\mccar/.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\mccar/.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n",
      "loading file spiece.model from cache at C:\\Users\\mccar/.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\spiece.model\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\mccar/.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\tokenizer_config.json\n",
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text preprocessed: \n",
      " don’t build your own MT system if you don’t have to. It is more practical to make use of the translation APIs. When we use such APIs, it is important to pay closer attention to pricing policies. It would perhaps make sense to store the translations of frequently used text (called a translation memory or a translation cache). If you’re working with a entirely new language, or say a new domain where existing translation APIs do poorly, it would make sense to start with a domain knowledge based rule based translation system addressing the restricted scenario you deal with. Another approach to address such data scarce scenarios is to augment your training data by doing “back translation”. Let us say we want to translate from English to Navajo language. English is a popular language for MT, but Navajo is not. We do have a few examples of English-Navajo translation. In such a case, one can build a first MT model between Navajo-English, and use this system to translate a few Navajo sentences into English. At this point, these machine translated Navajo-English pairs can be added as additional training data to English-Navajo MT system. This results in a translation system with more examples to train on (even though some of these examples are synthetic). In general, though, if accuracy of translation is paramount, it would perhaps make sense to form a hybrid MT system which combines the neural models with rules and some form of post-processing, though.\n",
      "\n",
      "\n",
      "Summarized text: \n",
      " it is more practical to make use of the translation APIs. if you’re working with a completely new language, it would make sense to store translations of frequently used text (called translation memory or translation cache)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "text =\"\"\"\n",
    "don’t build your own MT system if you don’t have to. It is more practical to make use of the translation APIs. When we use such APIs, it is important to pay closer attention to pricing policies. It would perhaps make sense to store the translations of frequently used text (called a translation memory or a translation cache). \n",
    "\n",
    "If you’re working with a entirely new language, or say a new domain where existing translation APIs do poorly, it would make sense to start with a domain knowledge based rule based translation system addressing the restricted scenario you deal with. Another approach to address such data scarce scenarios is to augment your training data by doing “back translation”. Let us say we want to translate from English to Navajo language. English is a popular language for MT, but Navajo is not. We do have a few examples of English-Navajo translation. In such a case, one can build a first MT model between Navajo-English, and use this system to translate a few Navajo sentences into English. At this point, these machine translated Navajo-English pairs can be added as additional training data to English-Navajo MT system. This results in a translation system with more examples to train on (even though some of these examples are synthetic). In general, though, if accuracy of translation is paramount, it would perhaps make sense to form a hybrid MT system which combines the neural models with rules and some form of post-processing, though. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
    "t5_prepared_Text = \"summarize: \"+preprocess_text\n",
    "print (\"original text preprocessed: \\n\", preprocess_text)\n",
    "\n",
    "tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "# summmarize \n",
    "summary_ids = model.generate(tokenized_text,\n",
    "                                    num_beams=4,\n",
    "                                    no_repeat_ngram_size=2,\n",
    "                                    min_length=30,\n",
    "                                    max_length=50,\n",
    "                                    early_stopping=True)\n",
    "#there are more parameters which can be found at https://huggingface.co/transformers/model_doc/t5.html\n",
    "\n",
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print (\"\\n\\nSummarized text: \\n\",output)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "TextSummarization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
