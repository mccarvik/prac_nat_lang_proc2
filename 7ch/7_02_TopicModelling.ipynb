{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "This notebook contains a demo of LDA and LSA using the gensim library. The dataset's link can be found in the `BookSummaries_Link.md` file under the Data folder in Ch7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OS \n",
    "import os\n",
    "# For NLTK virtual environments are high recommended and it requires python verisions higher than 3.5\n",
    "# !pip install gensim\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mccar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mccar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# stopwords = TfidfVectorizer(stop_words='english').get_stop_words()\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([(0.007278158, 'she'),\n",
      "   (0.00681306, 'he'),\n",
      "   (0.005864592, 'mother'),\n",
      "   (0.005447434, 'one'),\n",
      "   (0.004145695, 'life'),\n",
      "   (0.004128951, 'time'),\n",
      "   (0.0040128184, 'father'),\n",
      "   (0.004005924, 'family'),\n",
      "   (0.0039365673, 'home'),\n",
      "   (0.0038908832, 'school'),\n",
      "   (0.0038257958, 'new'),\n",
      "   (0.0037833296, 'back'),\n",
      "   (0.0036737144, 'tells'),\n",
      "   (0.003506203, 'also'),\n",
      "   (0.0034893362, 'they'),\n",
      "   (0.0032621392, 'go'),\n",
      "   (0.0032428734, 'when'),\n",
      "   (0.003189927, 'day'),\n",
      "   (0.0030907076, 'book'),\n",
      "   (0.0030232212, 'story')],\n",
      "  -0.9380709233842918),\n",
      " ([(0.0060619954, 'king'),\n",
      "   (0.0053629978, 'one'),\n",
      "   (0.005105551, 'world'),\n",
      "   (0.004732793, 'city'),\n",
      "   (0.004453306, 'he'),\n",
      "   (0.0041299853, 'they'),\n",
      "   (0.003975691, 'find'),\n",
      "   (0.003933881, 'in'),\n",
      "   (0.0035775476, 'army'),\n",
      "   (0.0033902617, 'way'),\n",
      "   (0.0033655926, 'back'),\n",
      "   (0.0032340211, 'battle'),\n",
      "   (0.0030773696, 'also'),\n",
      "   (0.0029530493, 'new'),\n",
      "   (0.0029476401, 'two'),\n",
      "   (0.0029461442, 'war'),\n",
      "   (0.002928865, 'power'),\n",
      "   (0.002919028, 'group'),\n",
      "   (0.0028637624, 'help'),\n",
      "   (0.002850658, 'people')],\n",
      "  -1.1295303388235896),\n",
      " ([(0.007763042, 'he'),\n",
      "   (0.006488428, 'father'),\n",
      "   (0.00530523, 'she'),\n",
      "   (0.0051622135, 'one'),\n",
      "   (0.00435279, 'love'),\n",
      "   (0.004189002, 'house'),\n",
      "   (0.004094376, 'tells'),\n",
      "   (0.0039577973, 'two'),\n",
      "   (0.0035317536, 'jacky'),\n",
      "   (0.0035014465, 'back'),\n",
      "   (0.0035000397, 'family'),\n",
      "   (0.0034432083, 'man'),\n",
      "   (0.0032956388, 'mother'),\n",
      "   (0.0032844618, 'wife'),\n",
      "   (0.0031917205, 'after'),\n",
      "   (0.0031606785, 'also'),\n",
      "   (0.0029660454, 'when'),\n",
      "   (0.0029393067, 'home'),\n",
      "   (0.0029092918, 'daughter'),\n",
      "   (0.002907348, 'finds')],\n",
      "  -1.1577588094930797),\n",
      " ([(0.006888008, 'he'),\n",
      "   (0.0038851283, 'one'),\n",
      "   (0.0037999998, 'family'),\n",
      "   (0.0036307438, 'life'),\n",
      "   (0.0036133276, 'story'),\n",
      "   (0.0035775767, 'two'),\n",
      "   (0.0034866135, 'back'),\n",
      "   (0.0034562796, 'time'),\n",
      "   (0.0034126572, 'new'),\n",
      "   (0.0032614272, 'michael'),\n",
      "   (0.003260625, 'she'),\n",
      "   (0.0032432189, 'first'),\n",
      "   (0.0031912895, 'in'),\n",
      "   (0.0031200165, 'man'),\n",
      "   (0.002917623, 'later'),\n",
      "   (0.0028376025, 'death'),\n",
      "   (0.0027255379, 'after'),\n",
      "   (0.0026722718, 'love'),\n",
      "   (0.0025824471, 'sarah'),\n",
      "   (0.0025683555, 'also')],\n",
      "  -1.345741611717747),\n",
      " ([(0.005590597, 'he'),\n",
      "   (0.0047543608, 'police'),\n",
      "   (0.0046334704, 'sam'),\n",
      "   (0.004352957, 'one'),\n",
      "   (0.0036946835, 'man'),\n",
      "   (0.0033330962, 'new'),\n",
      "   (0.0031727082, 'president'),\n",
      "   (0.0030511485, 'two'),\n",
      "   (0.003050993, 'in'),\n",
      "   (0.0030138565, 'back'),\n",
      "   (0.00298411, 'also'),\n",
      "   (0.0029806355, 'killed'),\n",
      "   (0.002936749, 'war'),\n",
      "   (0.0028454266, 'later'),\n",
      "   (0.0027274168, 'after'),\n",
      "   (0.0025480092, 'ben'),\n",
      "   (0.002501744, 'first'),\n",
      "   (0.002501503, 'american'),\n",
      "   (0.0024966833, 'novel'),\n",
      "   (0.0023269695, 'when')],\n",
      "  -1.3754359240518361),\n",
      " ([(0.010712287, 'book'),\n",
      "   (0.00799088, 'novel'),\n",
      "   (0.0058422675, 'also'),\n",
      "   (0.0051381825, 'in'),\n",
      "   (0.004605593, 'story'),\n",
      "   (0.004471437, 'he'),\n",
      "   (0.004200893, 'new'),\n",
      "   (0.004200846, 'one'),\n",
      "   (0.0040245177, 'chapter'),\n",
      "   (0.0039569195, 'life'),\n",
      "   (0.0036280993, 'first'),\n",
      "   (0.0035489388, 'people'),\n",
      "   (0.0035147779, 'world'),\n",
      "   (0.0034240582, 'war'),\n",
      "   (0.0030352632, 'author'),\n",
      "   (0.0028643226, 'it'),\n",
      "   (0.0028279577, 'this'),\n",
      "   (0.0028011193, 'work'),\n",
      "   (0.0027648082, 'political'),\n",
      "   (0.0026887367, 'two')],\n",
      "  -1.380615375151068),\n",
      " ([(0.0122324815, 'ship'),\n",
      "   (0.01017721, 'earth'),\n",
      "   (0.006222415, 'planet'),\n",
      "   (0.0061844196, 'space'),\n",
      "   (0.005906053, 'crew'),\n",
      "   (0.0051819813, 'human'),\n",
      "   (0.0051317373, 'they'),\n",
      "   (0.0047413497, 'new'),\n",
      "   (0.0044537983, 'alex'),\n",
      "   (0.0044268137, 'one'),\n",
      "   (0.004127674, 'alien'),\n",
      "   (0.0037624654, 'humans'),\n",
      "   (0.0034813834, 'system'),\n",
      "   (0.0033699826, 'time'),\n",
      "   (0.0030739813, 'mission'),\n",
      "   (0.0030429282, 'captain'),\n",
      "   (0.0029797824, 'ships'),\n",
      "   (0.0029664855, 'first'),\n",
      "   (0.0029604614, 'attack'),\n",
      "   (0.002837456, 'group')],\n",
      "  -1.6395063100537335),\n",
      " ([(0.0059368513, 'one'),\n",
      "   (0.004474541, 'he'),\n",
      "   (0.0038575728, 'ship'),\n",
      "   (0.0036738683, 'two'),\n",
      "   (0.0031799106, 'in'),\n",
      "   (0.0030568342, 'also'),\n",
      "   (0.0028918134, 'time'),\n",
      "   (0.002737342, 'story'),\n",
      "   (0.0026679768, 'however'),\n",
      "   (0.0025746066, 'novel'),\n",
      "   (0.002548408, 'world'),\n",
      "   (0.0025328132, 'captain'),\n",
      "   (0.0025001776, 'first'),\n",
      "   (0.0024982532, 'jack'),\n",
      "   (0.0023819518, 'new'),\n",
      "   (0.0023541485, 'life'),\n",
      "   (0.0022091842, 'years'),\n",
      "   (0.0021953064, 'father'),\n",
      "   (0.0021347827, 'richard'),\n",
      "   (0.002066684, 'arthur')],\n",
      "  -1.6939049111427056),\n",
      " ([(0.0053849733, 'he'),\n",
      "   (0.0048068864, 'back'),\n",
      "   (0.0047428734, 'one'),\n",
      "   (0.004715981, 'time'),\n",
      "   (0.003962187, 'also'),\n",
      "   (0.0035412433, 'new'),\n",
      "   (0.0032677804, 'luke'),\n",
      "   (0.003164553, 'after'),\n",
      "   (0.0030856, 'two'),\n",
      "   (0.0030461333, 'they'),\n",
      "   (0.0030052953, 'valkyrie'),\n",
      "   (0.0029843831, 'help'),\n",
      "   (0.0028881119, 'kill'),\n",
      "   (0.0028695546, 'team'),\n",
      "   (0.002795286, 'first'),\n",
      "   (0.0027817423, 'find'),\n",
      "   (0.0027702232, 'however'),\n",
      "   (0.002670844, 'ana'),\n",
      "   (0.0025155426, 'natalie'),\n",
      "   (0.0025035504, 'life')],\n",
      "  -2.8206342237350746),\n",
      " ([(0.021469142, 'david'),\n",
      "   (0.012093509, 'leo'),\n",
      "   (0.011667133, 'rachel'),\n",
      "   (0.010870777, 'mark'),\n",
      "   (0.007412657, 'martin'),\n",
      "   (0.0070147826, 'hugh'),\n",
      "   (0.006721895, 'dexter'),\n",
      "   (0.0058242125, 'toby'),\n",
      "   (0.0053559868, 'anne'),\n",
      "   (0.0051870765, 'malcolm'),\n",
      "   (0.0047152396, 'cass'),\n",
      "   (0.004557134, 'philip'),\n",
      "   (0.0045308955, 'he'),\n",
      "   (0.004530776, 'martha'),\n",
      "   (0.004358015, 'one'),\n",
      "   (0.0043291375, 'ash'),\n",
      "   (0.0040413504, 'will'),\n",
      "   (0.0039625573, 'bourne'),\n",
      "   (0.003893963, 'sara'),\n",
      "   (0.0037610156, 'she')],\n",
      "  -9.33295124902998)]\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "#tokenize, remove stopwords, non-alphabetic words, lowercase\n",
    "def preprocess(textstring):\n",
    "   stops =  set(stopwords.words('english'))\n",
    "   # stops = set(stopwords)\n",
    "   # print(textstring)\n",
    "   tokens = word_tokenize(textstring)\n",
    "   return [token.lower() for token in tokens if token.isalpha() and token not in stops]\n",
    "\n",
    "# This is a sample path of your downloaded data set. This is currently set to a windows based path . \n",
    "# Please update it to your actual download path regradless of your choice of operating system \n",
    "\n",
    "if False:\n",
    "   temp='../data/bigdata/booksumms/booksummaries.tar.gz' \n",
    "   tar = tarfile.open(temp, \"r:gz\")\n",
    "   tar.extractall('../data/bigdata/booksumms/')      \n",
    "   tar.close()\n",
    "data_path = '../data/bigdata/booksumms/booksummaries.txt' \n",
    "\n",
    "summaries = []\n",
    "for line in open(data_path, encoding=\"utf-8\"):\n",
    "   temp = line.split(\"\\t\")\n",
    "   summaries.append(preprocess(temp[6]))\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "\n",
    "dictionary = Dictionary(summaries)\n",
    "\n",
    "# Filter infrequent or too frequent words.\n",
    "\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(summary) for summary in summaries]\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "#Train the topic model\n",
    "\n",
    "model = LdaModel(corpus=corpus, id2word=id2word,iterations=400, num_topics=10)\n",
    "top_topics = list(model.top_topics(corpus))\n",
    "pprint(top_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: 0.006*\"he\" + 0.005*\"police\" + 0.005*\"sam\" + 0.004*\"one\" + 0.004*\"man\" + 0.003*\"new\" + 0.003*\"president\" + 0.003*\"two\" + 0.003*\"in\" + 0.003*\"back\"\n",
      "Topic #1: 0.012*\"ship\" + 0.010*\"earth\" + 0.006*\"planet\" + 0.006*\"space\" + 0.006*\"crew\" + 0.005*\"human\" + 0.005*\"they\" + 0.005*\"new\" + 0.004*\"alex\" + 0.004*\"one\"\n",
      "Topic #2: 0.007*\"he\" + 0.004*\"one\" + 0.004*\"family\" + 0.004*\"life\" + 0.004*\"story\" + 0.004*\"two\" + 0.003*\"back\" + 0.003*\"time\" + 0.003*\"new\" + 0.003*\"michael\"\n",
      "Topic #3: 0.021*\"david\" + 0.012*\"leo\" + 0.012*\"rachel\" + 0.011*\"mark\" + 0.007*\"martin\" + 0.007*\"hugh\" + 0.007*\"dexter\" + 0.006*\"toby\" + 0.005*\"anne\" + 0.005*\"malcolm\"\n",
      "Topic #4: 0.007*\"she\" + 0.007*\"he\" + 0.006*\"mother\" + 0.005*\"one\" + 0.004*\"life\" + 0.004*\"time\" + 0.004*\"father\" + 0.004*\"family\" + 0.004*\"home\" + 0.004*\"school\"\n",
      "Topic #5: 0.006*\"one\" + 0.004*\"he\" + 0.004*\"ship\" + 0.004*\"two\" + 0.003*\"in\" + 0.003*\"also\" + 0.003*\"time\" + 0.003*\"story\" + 0.003*\"however\" + 0.003*\"novel\"\n",
      "Topic #6: 0.008*\"he\" + 0.006*\"father\" + 0.005*\"she\" + 0.005*\"one\" + 0.004*\"love\" + 0.004*\"house\" + 0.004*\"tells\" + 0.004*\"two\" + 0.004*\"jacky\" + 0.004*\"back\"\n",
      "Topic #7: 0.006*\"king\" + 0.005*\"one\" + 0.005*\"world\" + 0.005*\"city\" + 0.004*\"he\" + 0.004*\"they\" + 0.004*\"find\" + 0.004*\"in\" + 0.004*\"army\" + 0.003*\"way\"\n",
      "Topic #8: 0.011*\"book\" + 0.008*\"novel\" + 0.006*\"also\" + 0.005*\"in\" + 0.005*\"story\" + 0.004*\"he\" + 0.004*\"new\" + 0.004*\"one\" + 0.004*\"chapter\" + 0.004*\"life\"\n",
      "Topic #9: 0.005*\"he\" + 0.005*\"back\" + 0.005*\"one\" + 0.005*\"time\" + 0.004*\"also\" + 0.004*\"new\" + 0.003*\"luke\" + 0.003*\"after\" + 0.003*\"two\" + 0.003*\"they\"\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(\"Topic #%s:\" % idx, model.print_topic(idx, 10))\n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.305*\"he\" + 0.215*\"one\" + 0.150*\"she\" + 0.140*\"time\" + 0.132*\"back\" + '\n",
      "  '0.131*\"also\" + 0.127*\"two\" + 0.125*\"they\" + 0.123*\"tells\" + 0.118*\"in\"'),\n",
      " (1,\n",
      "  '0.493*\"tom\" + 0.226*\"sophia\" + 0.182*\"mrs\" + 0.179*\"house\" + 0.160*\"she\" + '\n",
      "  '0.153*\"father\" + 0.147*\"mr\" + 0.146*\"he\" + 0.139*\"tells\" + -0.126*\"one\"'),\n",
      " (2,\n",
      "  '-0.557*\"tom\" + -0.252*\"sophia\" + 0.213*\"she\" + 0.190*\"he\" + -0.185*\"mrs\" + '\n",
      "  '0.163*\"tells\" + 0.143*\"mother\" + -0.136*\"mr\" + -0.130*\"western\" + '\n",
      "  '-0.102*\"however\"'),\n",
      " (3,\n",
      "  '-0.233*\"they\" + -0.201*\"ship\" + 0.186*\"he\" + -0.183*\"david\" + -0.178*\"back\" '\n",
      "  '+ -0.164*\"tells\" + 0.159*\"family\" + 0.158*\"life\" + -0.156*\"find\" + '\n",
      "  '0.153*\"narrator\"'),\n",
      " (4,\n",
      "  '0.663*\"he\" + -0.257*\"mother\" + -0.214*\"she\" + -0.196*\"father\" + '\n",
      "  '-0.182*\"family\" + 0.121*\"narrator\" + 0.120*\"monk\" + -0.100*\"novel\" + '\n",
      "  '-0.096*\"school\" + -0.095*\"children\"'),\n",
      " (5,\n",
      "  '-0.487*\"david\" + 0.242*\"king\" + -0.168*\"rosa\" + -0.165*\"book\" + '\n",
      "  '-0.125*\"harlan\" + 0.120*\"he\" + -0.114*\"she\" + -0.109*\"gould\" + '\n",
      "  '0.108*\"anita\" + -0.106*\"would\"'),\n",
      " (6,\n",
      "  '0.698*\"anita\" + 0.469*\"richard\" + -0.157*\"ship\" + -0.131*\"jacky\" + '\n",
      "  '0.086*\"edward\" + 0.084*\"power\" + 0.079*\"monk\" + 0.073*\"scene\" + '\n",
      "  '0.071*\"kill\" + -0.070*\"father\"'),\n",
      " (7,\n",
      "  '0.398*\"david\" + 0.357*\"king\" + -0.224*\"jacky\" + -0.190*\"ship\" + '\n",
      "  '-0.144*\"monk\" + -0.135*\"doctor\" + 0.129*\"rosa\" + 0.123*\"arthur\" + '\n",
      "  '0.121*\"prince\" + 0.113*\"book\"'),\n",
      " (8,\n",
      "  '0.283*\"harry\" + -0.280*\"she\" + 0.260*\"narrator\" + -0.234*\"jacky\" + '\n",
      "  '0.226*\"david\" + 0.199*\"monk\" + 0.145*\"natalie\" + -0.126*\"ship\" + '\n",
      "  '-0.124*\"says\" + 0.118*\"alex\"'),\n",
      " (9,\n",
      "  '-0.459*\"harry\" + 0.410*\"narrator\" + -0.263*\"monk\" + 0.220*\"david\" + '\n",
      "  '0.214*\"anita\" + -0.183*\"natalie\" + 0.168*\"ship\" + 0.158*\"he\" + '\n",
      "  '0.152*\"richard\" + -0.148*\"dresden\"')]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LsiModel\n",
    "lsamodel = LsiModel(corpus, num_topics=10, id2word = id2word)  # train model\n",
    "\n",
    "pprint(lsamodel.print_topics(num_topics=10, num_words=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: 0.305*\"he\" + 0.215*\"one\" + 0.150*\"she\" + 0.140*\"time\" + 0.132*\"back\" + 0.131*\"also\" + 0.127*\"two\" + 0.125*\"they\" + 0.123*\"tells\" + 0.118*\"in\"\n",
      "Topic #1: 0.493*\"tom\" + 0.226*\"sophia\" + 0.182*\"mrs\" + 0.179*\"house\" + 0.160*\"she\" + 0.153*\"father\" + 0.147*\"mr\" + 0.146*\"he\" + 0.139*\"tells\" + -0.126*\"one\"\n",
      "Topic #2: -0.557*\"tom\" + -0.252*\"sophia\" + 0.213*\"she\" + 0.190*\"he\" + -0.185*\"mrs\" + 0.163*\"tells\" + 0.143*\"mother\" + -0.136*\"mr\" + -0.130*\"western\" + -0.102*\"however\"\n",
      "Topic #3: -0.233*\"they\" + -0.201*\"ship\" + 0.186*\"he\" + -0.183*\"david\" + -0.178*\"back\" + -0.164*\"tells\" + 0.159*\"family\" + 0.158*\"life\" + -0.156*\"find\" + 0.153*\"narrator\"\n",
      "Topic #4: 0.663*\"he\" + -0.257*\"mother\" + -0.214*\"she\" + -0.196*\"father\" + -0.182*\"family\" + 0.121*\"narrator\" + 0.120*\"monk\" + -0.100*\"novel\" + -0.096*\"school\" + -0.095*\"children\"\n",
      "Topic #5: -0.487*\"david\" + 0.242*\"king\" + -0.168*\"rosa\" + -0.165*\"book\" + -0.125*\"harlan\" + 0.120*\"he\" + -0.114*\"she\" + -0.109*\"gould\" + 0.108*\"anita\" + -0.106*\"would\"\n",
      "Topic #6: 0.698*\"anita\" + 0.469*\"richard\" + -0.157*\"ship\" + -0.131*\"jacky\" + 0.086*\"edward\" + 0.084*\"power\" + 0.079*\"monk\" + 0.073*\"scene\" + 0.071*\"kill\" + -0.070*\"father\"\n",
      "Topic #7: 0.398*\"david\" + 0.357*\"king\" + -0.224*\"jacky\" + -0.190*\"ship\" + -0.144*\"monk\" + -0.135*\"doctor\" + 0.129*\"rosa\" + 0.123*\"arthur\" + 0.121*\"prince\" + 0.113*\"book\"\n",
      "Topic #8: 0.283*\"harry\" + -0.280*\"she\" + 0.260*\"narrator\" + -0.234*\"jacky\" + 0.226*\"david\" + 0.199*\"monk\" + 0.145*\"natalie\" + -0.126*\"ship\" + -0.124*\"says\" + 0.118*\"alex\"\n",
      "Topic #9: -0.459*\"harry\" + 0.410*\"narrator\" + -0.263*\"monk\" + 0.220*\"david\" + 0.214*\"anita\" + -0.183*\"natalie\" + 0.168*\"ship\" + 0.158*\"he\" + 0.152*\"richard\" + -0.148*\"dresden\"\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(\"Topic #%s:\" % idx, lsamodel.print_topic(idx, 10))\n",
    " \n",
    "print(\"=\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
