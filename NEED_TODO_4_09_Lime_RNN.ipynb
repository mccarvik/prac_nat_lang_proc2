{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjoUGDuyTdh5"
      },
      "source": [
        "In this notebook we will demonstrate how to interpret a Deep Learning Model using [LIME](https://github.com/marcotcr/lime)(local interpretable model-agnostic explanations), a python package for explaining machine learning classifiers. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVQdTkPzH-Y7",
        "outputId": "1d0dea01-69b6-4c3b-930d-5d0bf45b6c96"
      },
      "outputs": [],
      "source": [
        "# To install only the requirements of this notebook, uncomment the lines below and run this cell\n",
        "\n",
        "# ===========================\n",
        "\n",
        "# !pip install pandas==1.1.5\n",
        "# !pip install scikit-learn==0.21.3\n",
        "# !pip install lime==0.2.0.1\n",
        "# !pip install tensorflow==1.14.0\n",
        "# !pip install numpy==1.19.5\n",
        "# !pip install matplotlib==3.2.2\n",
        "# !pip install seaborn==0.11.1\n",
        "\n",
        "# ==========================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xQa4Mi5HH-Y8"
      },
      "outputs": [],
      "source": [
        "# To install the requirements for the entire chapter, uncomment the lines below and run this cell\n",
        "\n",
        "# ===========================\n",
        "\n",
        "# try:\n",
        "#     import google.colab\n",
        "#     !curl  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/ch4-requirements.txt | xargs -n 1 -L 1 pip install\n",
        "# except ModuleNotFoundError:\n",
        "#     !pip install -r \"ch4-requirements.txt\"\n",
        "\n",
        "# ==========================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5S86uIWZKVO"
      },
      "source": [
        "We will be building an LSTM model with an embedding layer trained on the fly. We will be following all the preprocessing steps as in the [DeepNN_Example.ipynb](https://github.com/practical-nlp/practical-nlp/blob/master/Ch4/DeepNN_Example.ipynb) notebook in this repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "UsCn1xlo_MMX"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "#helper functions to lead the data\n",
        "def load_directory_data(directory):\n",
        "    data = {}\n",
        "    data[\"sentence\"] = []\n",
        "    data[\"sentiment\"] = []\n",
        "    for file_path in os.listdir(directory):\n",
        "        with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "            data[\"sentence\"].append(f.read())\n",
        "            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
        "    return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(directory):\n",
        "    pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "    neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "    pos_df[\"polarity\"] = 1\n",
        "    neg_df[\"polarity\"] = 0\n",
        "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "def download_and_load_datasets(force_download=False):\n",
        "    dataset = tf.keras.utils.get_file(\n",
        "      fname=\"aclImdb.tar.gz\", \n",
        "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "      extract=True)\n",
        "\n",
        "    train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                       \"aclImdb\", \"train\"))\n",
        "    test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                      \"aclImdb\", \"test\"))\n",
        "  \n",
        "    return train_df, test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "dlBTwerPX1EV"
      },
      "outputs": [],
      "source": [
        "# try:\n",
        "#     from google.colab import files\n",
        "    \n",
        "#     if not os.path.exists('aclImdb'):\n",
        "#         train,test = download_and_load_datasets()\n",
        "#     else:\n",
        "#         train = load_dataset('aclImdb/train')\n",
        "#         test = load_dataset('aclImdb/test')\n",
        "    \n",
        "# except ModuleNotFoundError:\n",
        "#     # if not os.path.exists('data/bigdata/aclImdb'):\n",
        "#     #     train,test = download_and_load_datasets()\n",
        "#     if False:\n",
        "#         train,test = download_and_load_datasets()\n",
        "#     else:\n",
        "#         train = load_dataset('data/bigdata/aclImdb/train')\n",
        "#         test = load_dataset('Ddata/bigdata/aclImdb/test')\n",
        "train = load_dataset('data/bigdata/aclImdb/train')\n",
        "test = load_dataset('data/bigdata/aclImdb/test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hEpQWHnF-hOX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 20000 \n",
        "EMBEDDING_DIM = 100 \n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "vocab_size = 20000  # Max number of different word, i.e. model input dimension\n",
        "maxlen = 1000 # Max number of words kept at the end of each text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "830AVGaZBfnf",
        "outputId": "1a80608b-b295-41c3-dc2f-bfbd701667d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['sentence', 'sentiment', 'polarity'], dtype='object')"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "FJo_FLISBhx6"
      },
      "outputs": [],
      "source": [
        "train_texts = train['sentence'].values\n",
        "train_labels = train['polarity'].values\n",
        "test_texts = test['sentence'].values\n",
        "# test_labels = test['polarity'].values\n",
        "\n",
        "labels_index = {'pos':1, 'neg':0} "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ1G3DH3dDQ4",
        "outputId": "6aee306f-91e4-45a3-fae1-db00d530ff2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['sentence', 'sentiment', 'polarity'], dtype='object')"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8I6QgXldKd0",
        "outputId": "e96585e8-3a61-45f1-8873-247b4b6afa13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 0, 1, 1], dtype=int64)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_labels = test['polarity'].values\n",
        "test_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfPAZEEKajly"
      },
      "source": [
        "We need to design an sklearn pipeline with our model.\n",
        "What is a pipeline?<br>\n",
        "\n",
        "**Transformer** in scikit-learn - some class that have fit and transform method, or fit_transform method.\n",
        "\n",
        "**Predictor** - some class that has fit and predict methods, or fit_predict method.\n",
        "\n",
        "**Pipeline** is just an abstract notion, it's not some existing ml algorithm. Often in ML tasks you need to perform sequence of different transformations (find set of features, generate new features, select only some good features) of raw dataset before applying final estimator. Pipeline gives you a single interface for all 3 steps of transformation and resulting estimator. It encapsulates transformers and predictors inside"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ibekacAMMTsr"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.pipeline import TransformerMixin\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "\n",
        "class TextsToSequences(Tokenizer, BaseEstimator, TransformerMixin):\n",
        "    \"\"\" Sklearn transformer to convert texts to indices list \n",
        "    (e.g. [[\"the cute cat\"], [\"the dog\"]] -> [[1, 2, 3], [1, 4]])\"\"\"\n",
        "    def __init__(self,  **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "    def fit(self, texts, y=None):\n",
        "        self.fit_on_texts(texts)\n",
        "        return self\n",
        "    \n",
        "    def transform(self, texts, y=None):\n",
        "        return np.array(self.texts_to_sequences(texts))\n",
        "        \n",
        "sequencer = TextsToSequences(num_words=vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Padder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\" Pad and crop uneven lists to the same length. \n",
        "    Only the end of lists longernthan the maxlen attribute are\n",
        "    kept, and lists shorter than maxlen are left-padded with zeros\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    maxlen: int\n",
        "        sizes of sequences after padding\n",
        "    max_index: int\n",
        "        maximum index known by the Padder, if a higher index is met during \n",
        "        transform it is transformed to a 0\n",
        "    \"\"\"\n",
        "    def __init__(self, maxlen=500):\n",
        "        self.maxlen = maxlen\n",
        "        self.max_index = None\n",
        "        \n",
        "    def fit(self, X, y=None):\n",
        "        self.max_index = pad_sequences(X, maxlen=self.maxlen).max()\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "        X = pad_sequences(X, maxlen=self.maxlen)\n",
        "        X[X > self.max_index] = 0\n",
        "        return X\n",
        "\n",
        "padder = Padder(maxlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mtEN5BjbyKM"
      },
      "source": [
        "We will only train for 2 epochs. A better model could be trained with more epochs and early stopping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XvXEYGBMWlW",
        "outputId": "4ea00082-b2b0-4760-b083-77b033daa59b"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (25000,) + inhomogeneous part.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[38], line 35\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39m# sklearn_lstm = KerasClassifier(build_fn=create_model, epochs=2, batch_size=32, \u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m#                                max_features=max_features, verbose=1)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m \u001b[39m# Build the Scikit-learn pipeline\u001b[39;00m\n\u001b[0;32m     33\u001b[0m pipeline \u001b[39m=\u001b[39m make_pipeline(sequencer, padder, sklearn_lstm)\n\u001b[1;32m---> 35\u001b[0m pipeline\u001b[39m.\u001b[39;49mfit(train_texts, train_labels);\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py:416\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \n\u001b[0;32m    392\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    415\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m--> 416\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps)\n\u001b[0;32m    417\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[0;32m    418\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py:370\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    368\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[0;32m    369\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 370\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    371\u001b[0m     cloned_transformer,\n\u001b[0;32m    372\u001b[0m     X,\n\u001b[0;32m    373\u001b[0m     y,\n\u001b[0;32m    374\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    375\u001b[0m     message_clsname\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    376\u001b[0m     message\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    377\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    379\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py:950\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    949\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 950\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit_transform(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    951\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    915\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    916\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39;49mtransform(X)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
            "Cell \u001b[1;32mIn[34], line 18\u001b[0m, in \u001b[0;36mTextsToSequences.transform\u001b[1;34m(self, texts, y)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, texts, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 18\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49marray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtexts_to_sequences(texts))\n",
            "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (25000,) + inhomogeneous part."
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM\n",
        "# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "batch_size = 64\n",
        "max_features = vocab_size + 1\n",
        "\n",
        "#Training an LSTM with embedding on the fly\n",
        "def create_model(max_features):\n",
        "    \"\"\" Model creation function: returns a compiled LSTM\"\"\"\n",
        "\n",
        "\n",
        "    rnnmodel = Sequential()\n",
        "    rnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
        "    rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    rnnmodel.add(Dense(1, activation='sigmoid'))\n",
        "    rnnmodel.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "    return rnnmodel\n",
        "\n",
        "\n",
        "# Use Keras Scikit-learn wrapper to instantiate a LSTM with all methods\n",
        "# required by Scikit-learn for the last step of a Pipeline\n",
        "sklearn_lstm = create_model(0)\n",
        "# sklearn_lstm = KerasClassifier(build_fn=create_model, epochs=2, batch_size=32, \n",
        "#                                max_features=max_features, verbose=1)\n",
        "\n",
        "# Build the Scikit-learn pipeline\n",
        "pipeline = make_pipeline(sequencer, padder, sklearn_lstm)\n",
        "\n",
        "pipeline.fit(train_texts, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTph3cZ2anzx",
        "outputId": "d469ca0d-836c-4911-a194-3f9b01253b94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 201s 8ms/sample\n"
          ]
        }
      ],
      "source": [
        "y_preds = pipeline.predict(test_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KKnIf5BcnQn",
        "outputId": "5f7f975d-d7da-4618-f63d-8f9ac3162c97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 79.89 %\n"
          ]
        }
      ],
      "source": [
        "print('Test accuracy: {:.2f} %'.format(100*accuracy_score(y_preds, test_labels)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "jd9mzgXrZ9ys",
        "outputId": "28ee9c4e-f38f-4b42-e7b2-b032230892b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 11: last 1000 words (only part used by the model)\n",
            "--------------------------------------------------\n",
            "I had the privilege of being one of the Still photographers on the set of \"Grand Champion\" and enjoyed every minute of the 42 days I worked on the movie. I have been in the Photography business for 25 years and have worked on 16 movies and I can't think of a time when I enjoyed providing my craft more. The Kids were wonderful to work with and little Emma Roberts has so much energy she's a real trip. She even grabbed one of my camera during the stockshow scene rehearsal and started shooting. Some of her images were used for PR. I could have made more money working for a production with a bigger budget but I doubt I would have had the fun and been around so many great actors and the great people of West Texas as I was.\n",
            "--------------------------------------------------\n",
            "\r1/1 [==============================] - 0s 125ms/sample\n",
            "Probability(positive) = 0.43980047\n",
            "True class: positive\n",
            "5000/5000 [==============================] - 40s 8ms/sample\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# We choose a sample from test set\n",
        "idx = 11\n",
        "text_sample = test_texts[idx]\n",
        "class_names = ['negative', 'positive']\n",
        "\n",
        "print('Sample {}: last 1000 words (only part used by the model)'.format(idx))\n",
        "print('-'*50)\n",
        "print(\" \".join(text_sample.split()[-1000:]))\n",
        "print('-'*50)\n",
        "print('Probability(positive) =', pipeline.predict_proba([text_sample])[0,1])\n",
        "print('True class: %s' % class_names[test_labels[idx]])\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from collections import OrderedDict\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "explainer = LimeTextExplainer(class_names=class_names)\n",
        "explanation = explainer.explain_instance(text_sample, pipeline.predict_proba, num_features=10)\n",
        "\n",
        "weights = OrderedDict(explanation.as_list())\n",
        "lime_weights = pd.DataFrame({'words': list(weights.keys()), 'weights': list(weights.values())})\n",
        "\n",
        "sns.barplot(x=\"words\", y=\"weights\", data=lime_weights);\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('Sample {} features weights given by LIME'.format(idx));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tdpfNvHcaDB"
      },
      "source": [
        "We have used the LIME interpretation to provide explanations for a recurrent neural network. Looking at the graph we understand that the sentence is negative and the word \"worst\" affects it the most.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "09_Lime_RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
